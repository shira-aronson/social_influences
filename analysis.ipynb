{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aeb0835-6019-412d-b390-628f8cf514ca",
   "metadata": {},
   "source": [
    "This file includes the analysis conducted on the pornography consumption survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f7639e-c69d-4024-bb30-e383ab236d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb87fc-eb14-4c56-a298-eb36b0b67ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the dataset\n",
    "data = pd.read_csv(\"\")  # File path can be added here for confidentiality.\n",
    "data = data[~(data['Status'] == \"Survey Preview\")]  # Remove non-participant rows\n",
    "data = data[data['Consent'] == \"I agree to participate in the research\"]  # Keep only rows where participants consented\n",
    "\n",
    "# Drop unnecessary metadata columns\n",
    "columns_to_drop = [\"UserLanguage\", \"StartDate\", \"EndDate\", \"Status\", \"DistributionChannel\", \n",
    "                   \"Q_RecaptchaScore\", \"Q37_Click.Count\", \"Q38_Click.Count\", \"Q39_Click.Count\", \n",
    "                   \"Q37_Last.Click\", \"Q38_Last.Click\", \"Q39_Last.Click\"]\n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Replace empty string entries with NaN for further cleaning\n",
    "data.replace(\"\", np.nan, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2ffca-9e18-4730-b9c0-4d7dac1487e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "rename_dict = {\n",
    "    'Q1': 'age',\n",
    "    'Q2': 'ethnicity',\n",
    "    'Q3_1': 'religious_affiliation',\n",
    "    'Q4': 'education_completed',\n",
    "    'Q5': 'education_region',\n",
    "    'Q6': 'alcohol_per_week',\n",
    "    'Q7': 'gender_identity',\n",
    "    'Q40_1': 'pre_test_feelings',\n",
    "    'Q37_First.Click': 'control_group_start',\n",
    "    'Q37_Page.Submit': 'control_group_end',\n",
    "    'Q38_First.Click': 'info_group_start',\n",
    "    'Q38_Page.Submit': 'info_group_end',\n",
    "    'Q39_First.Click': 'pre_test_start',\n",
    "    'Q39_Page.Submit': 'pre_test_end',\n",
    "    'Q36_1': 'info_group_feelings',\n",
    "    'Q10': 'discomfort_scenario_A',\n",
    "    'Q11': 'discomfort_scenario_B',\n",
    "    'Q12': 'discomfort_scenario_C',\n",
    "    'Q13': 'discomfort_scenario_D',\n",
    "    'Q14': 'discomfort_scenario_E',\n",
    "    'Q15': 'discomfort_scenario_F',\n",
    "    'Q16': 'unclear_scenario_G',\n",
    "    'Q16.1': 'num_partners',\n",
    "    'Q17_1': 'sexual_orientation',\n",
    "    'Q17_1.1': 'frequency_of_sex',\n",
    "    'Q18': 'age_first_exposure',\n",
    "    'Q19_1': 'incident_report',\n",
    "    'Q20': 'relationship_status',\n",
    "    'Q21_1': 'frequency_of_activity_A',\n",
    "    'Q22_1': 'frequency_of_activity_B',\n",
    "    'Q23_1': 'activity_A_without_activity_B',\n",
    "    'Q24_1': 'comfort_discussing_topic_A',\n",
    "    'Q25_1': 'discomfort_discussing_topic_A',\n",
    "    'Q26_1': 'never_discussed_activity_A',\n",
    "    'Q27_1': 'comfort_discussing_activity_B',\n",
    "    'Q28_1': 'activity_B_with_partner',\n",
    "    'Q29_1': 'unaware_activity_B_made',\n",
    "    'Q30_1': 'guilt_feelings',\n",
    "    'Q31_1': 'appeal_level',\n",
    "    'Q32_1': 'awareness_of_creation',\n",
    "    'Q33_1': 'education_about_topic',\n",
    "    'Q34_1': 'friends_comfortable',\n",
    "    'Q35_1': 'people_comfortable'\n",
    "}\n",
    "\n",
    "# Apply renaming\n",
    "data.rename(columns=rename_dict, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b9b49-4566-4537-bf2c-e0e60bb42058",
   "metadata": {},
   "source": [
    "In the experiment, participants in the control and treatment groups describe their comfort with a variety of sexually-explicit vignettes. Some vignettes describe consensual scenarios, while others are non-consensual or unclear. In the following sections, scenarios are grouped by consent level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c9473-4eaf-4bd3-b728-ea9e78267314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate comfort level in non-consensual scenarios.\n",
    "\n",
    "# Define a function to plot comfort levels and save to PDF\n",
    "def plot_comfort_levels(column, title, filename):\n",
    "    # Count the values for the given column and fill missing categories with 0\n",
    "    counts = data[column].value_counts().reindex(\n",
    "        [\"Very uncomfortable\", \"Uncomfortable\", \"Neither comfortable nor uncomfortable\", \n",
    "         \"Comfortable\", \"Very comfortable\"], fill_value=0)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=counts.index, y=counts.values)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Comfort Level\")\n",
    "    plt.ylabel(\"Number of Responses\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot as a PDF file\n",
    "    pdf_filename = f\"{filename}.pdf\"\n",
    "    plt.savefig(pdf_filename)\n",
    "    print(f\"Saved: {pdf_filename}\")\n",
    "    plt.close()\n",
    "\n",
    "# List of columns, their corresponding titles, and filenames for PDFs\n",
    "scenarios = [\n",
    "    ('discomfort_scenario_A', 'Aggregate Comfort Level with Non-Consensual Scenario A', 'non_consent_scenario_A_comfort_level'),\n",
    "    ('discomfort_scenario_F', 'Aggregate Comfort Level with Non-Consensual Scenario F', 'non_consent_scenario_F_comfort_level'),\n",
    "    ('discomfort_scenario_E', 'Aggregate Comfort Level with Non-Consensual Scenario E', 'non_consent_scenario_E_comfort_level')\n",
    "]\n",
    "\n",
    "# Loop through each scenario, plot, and save as PDF\n",
    "for column, title, filename in scenarios:\n",
    "    plot_comfort_levels(column, title, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f75fa2-778c-4a28-b16c-bb35e3f16a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined plot of comfort levels in non-consensual scenarios\n",
    "\n",
    "# Import GridSpec for arranging plots in a grid\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Create a function to plot multiple scenarios in one figure\n",
    "def combined_plot_scenarios():\n",
    "    # Create subplots with GridSpec\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    grid = GridSpec(1, 3, figure=fig)\n",
    "\n",
    "    # Plot for Scenario A\n",
    "    ax1 = fig.add_subplot(grid[0, 0])\n",
    "    counts_scenario_A = data['discomfort_scenario_A'].value_counts().reindex(\n",
    "        [\"Very uncomfortable\", \"Uncomfortable\", \"Neither comfortable nor uncomfortable\", \n",
    "         \"Comfortable\", \"Very comfortable\"], fill_value=0)\n",
    "    sns.barplot(x=counts_scenario_A.index, y=counts_scenario_A.values, ax=ax1)\n",
    "    ax1.set_title('a. Scenario A')\n",
    "    ax1.set_xlabel('Comfort Level')\n",
    "    ax1.set_ylabel('Number of Responses')\n",
    "    ax1.set_xticklabels(counts_scenario_A.index, rotation=45)\n",
    "\n",
    "    # Plot for Scenario F\n",
    "    ax2 = fig.add_subplot(grid[0, 1])\n",
    "    counts_scenario_F = data['discomfort_scenario_F'].value_counts().reindex(\n",
    "        [\"Very uncomfortable\", \"Uncomfortable\", \"Neither comfortable nor uncomfortable\", \n",
    "         \"Comfortable\", \"Very comfortable\"], fill_value=0)\n",
    "    sns.barplot(x=counts_scenario_F.index, y=counts_scenario_F.values, ax=ax2)\n",
    "    ax2.set_title('b. Scenario F')\n",
    "    ax2.set_xlabel('Comfort Level')\n",
    "    ax2.set_ylabel('')\n",
    "    ax2.set_xticklabels(counts_scenario_F.index, rotation=45)\n",
    "\n",
    "    # Plot for Scenario E\n",
    "    ax3 = fig.add_subplot(grid[0, 2])\n",
    "    counts_scenario_E = data['discomfort_scenario_E'].value_counts().reindex(\n",
    "        [\"Very uncomfortable\", \"Uncomfortable\", \"Neither comfortable nor uncomfortable\", \n",
    "         \"Comfortable\", \"Very comfortable\"], fill_value=0)\n",
    "    sns.barplot(x=counts_scenario_E.index, y=counts_scenario_E.values, ax=ax3)\n",
    "    ax3.set_title('c. Scenario E')\n",
    "    ax3.set_xlabel('Comfort Level')\n",
    "    ax3.set_ylabel('')\n",
    "    ax3.set_xticklabels(counts_scenario_E.index, rotation=45)\n",
    "\n",
    "    # Add a title to the overall figure\n",
    "    fig.suptitle(\"Figure 2: Comfort with Non-Consensual Scenarios\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Save the combined plot as a PDF\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit title\n",
    "    plt.savefig(\"combined_non_consent_scenarios.pdf\")\n",
    "    print(\"Saved: combined_non_consent_scenarios.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to generate and save the combined plot\n",
    "combined_plot_scenarios()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce585d-9801-402a-9c84-00db0f51f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Cisgender female participants\n",
    "data_female = data[data['gender_identity'] == 'Cisgender female'].copy()\n",
    "\n",
    "# Convert sexual_orientation to numeric\n",
    "data_female['sexual_orientation'] = pd.to_numeric(data_female['sexual_orientation'], errors='coerce')\n",
    "\n",
    "# Create a lookup dictionary to map comfort levels to numeric values\n",
    "comfort_lookup = {\n",
    "    \"Very uncomfortable\": 1,\n",
    "    \"Uncomfortable\": 2,\n",
    "    \"Neither comfortable nor uncomfortable\": 3,\n",
    "    \"Comfortable\": 4,\n",
    "    \"Very comfortable\": 5\n",
    "}\n",
    "\n",
    "# Use the lookup table to replace string values with numeric values for specific columns\n",
    "data_female['discomfort_scenario_F'] = data_female['discomfort_scenario_F'].map(comfort_lookup)\n",
    "data_female['discomfort_scenario_E'] = data_female['discomfort_scenario_E'].map(comfort_lookup)\n",
    "\n",
    "# Subset participants into two groups based on sexual orientation\n",
    "group_homosexual = data_female[data_female['sexual_orientation'] >= 3].copy()\n",
    "group_heterosexual = data_female[data_female['sexual_orientation'] < 3].copy()\n",
    "\n",
    "# Drop NaN values in 'discomfort_scenario_F' for both groups\n",
    "group_homosexual = group_homosexual.dropna(subset=['discomfort_scenario_F'])\n",
    "group_heterosexual = group_heterosexual.dropna(subset=['discomfort_scenario_F'])\n",
    "\n",
    "# Display summary statistics for the discomfort in Scenario F\n",
    "print(group_homosexual['discomfort_scenario_F'].describe())\n",
    "print(group_heterosexual['discomfort_scenario_F'].describe())\n",
    "\n",
    "# Calculate the sample mean and standard error for 'discomfort_scenario_F' in the homosexual group\n",
    "sample_mean_homosexual = group_homosexual['discomfort_scenario_F'].mean()\n",
    "se_homosexual = group_homosexual['discomfort_scenario_F'].std() / np.sqrt(len(group_homosexual))\n",
    "\n",
    "print(f\"Sample Mean (Homosexual group): {sample_mean_homosexual}\")\n",
    "print(f\"Standard Error (Homosexual group): {se_homosexual}\")\n",
    "\n",
    "# Calculate the z-score and p-value for the difference in means between the two groups\n",
    "mean_heterosexual = group_heterosexual['discomfort_scenario_F'].mean()\n",
    "z_score = (sample_mean_homosexual - mean_heterosexual) / se_homosexual\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
    "\n",
    "print(f\"Z-Score: {z_score}\")\n",
    "print(f\"P-Value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ccff3d-67d6-4db4-a9ae-49dbba1f1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine rates of non-answering for participants based on sexual orientation\n",
    "\n",
    "# Filter out rows where Progress is 100%\n",
    "data_complete = data[data['Progress'] == 100]\n",
    "\n",
    "# Subset for Cisgender female participants\n",
    "data_female_complete = data_complete[data_complete['gender_identity'] == 'Cisgender female']\n",
    "\n",
    "# Subset for homosexual (sexual_orientation >= 3) and heterosexual (sexual_orientation < 3) groups\n",
    "group_homosexual = data_female_complete[data_female_complete['sexual_orientation'] >= 3]\n",
    "group_heterosexual = data_female_complete[data_female_complete['sexual_orientation'] < 3]\n",
    "\n",
    "# List of columns to check for missing values (NaNs)\n",
    "cols_to_check = [\"discomfort_scenario_A\", \"discomfort_scenario_F\", \"discomfort_scenario_E\", \n",
    "                 \"discomfort_scenario_B\", \"discomfort_scenario_C\", \"discomfort_scenario_D\", \"unclear_scenario_G\"]\n",
    "\n",
    "# Calculate the proportion of NaN values for each column in both subsets\n",
    "prop_na_homosexual = group_homosexual[cols_to_check].isna().mean()\n",
    "prop_na_heterosexual = group_heterosexual[cols_to_check].isna().mean()\n",
    "\n",
    "# Create a DataFrame to compare the proportions\n",
    "prop_na_df = pd.DataFrame({\n",
    "    'category': cols_to_check,\n",
    "    'homosexual_prop_na': prop_na_homosexual.values,\n",
    "    'heterosexual_prop_na': prop_na_heterosexual.values\n",
    "})\n",
    "\n",
    "# View the proportions\n",
    "print(prop_na_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5d2e9-46ad-4b92-ab17-2cee7ee3e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat plotting for consensual scenarios.\n",
    "\n",
    "# Function to create bar plot for each scenario\n",
    "def plot_comfort_scenario(data, column, title, ax):\n",
    "    # Create a target order for the comfort levels\n",
    "    target_order = [\"Very uncomfortable\", \"Uncomfortable\", \"Neither comfortable nor uncomfortable\", \n",
    "                    \"Comfortable\", \"Very comfortable\"]\n",
    "    \n",
    "    # Count the values for each comfort level, filling missing categories with 0\n",
    "    counts = data[column].value_counts().reindex(target_order, fill_value=0)\n",
    "    \n",
    "    # Create the bar plot on the given axis\n",
    "    sns.barplot(x=counts.index, y=counts.values, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Comfort Level')\n",
    "    ax.set_ylabel('Number of Responses')\n",
    "    ax.set_xticklabels(counts.index, rotation=45)\n",
    "\n",
    "# Set up the figure and GridSpec for arranging plots\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "grid = GridSpec(1, 3, figure=fig)\n",
    "\n",
    "# Plot for Scenario Sam (consensual)\n",
    "ax1 = fig.add_subplot(grid[0, 0])\n",
    "plot_comfort_scenario(data, 'comfort_scenario_Sam', 'Aggregate Comfort Level with Consensual Scenario (Sam)', ax1)\n",
    "\n",
    "# Plot for Scenario Dan\n",
    "ax2 = fig.add_subplot(grid[0, 1])\n",
    "plot_comfort_scenario(data, 'comfort_scenario_Dan', 'Aggregate Comfort Level with Consensual Scenario (Dan)', ax2)\n",
    "\n",
    "# Plot for Scenario Rebecca\n",
    "ax3 = fig.add_subplot(grid[0, 2])\n",
    "plot_comfort_scenario(data, 'comfort_scenario_Rebecca', 'Aggregate Comfort Level with Consensual Scenario (Rebecca)', ax3)\n",
    "\n",
    "# Set the overall title for the figure\n",
    "fig.suptitle(\"Figure 3: Comfort with Consensual Scenarios\", fontsize=16, fontweight='bold')\n",
    "\n",
    "# Save the combined plot as a PDF\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit title\n",
    "plt.savefig(\"combined_consent_scenarios.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855d7bf-7839-4f5e-9c56-a8314d7a8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recreate above plot with small tweaks\n",
    "\n",
    "# Filter out rows with NaNs\n",
    "data_filtered_sam = data[~data['comfort_scenario_Sam'].isna()]\n",
    "data_filtered_dan = data[~data['comfort_scenario_Dan'].isna()]\n",
    "data_filtered_rebecca = data[~data['comfort_scenario_Rebecca'].isna()]\n",
    "\n",
    "# Function to create filtered bar plots\n",
    "def plot_filtered_scenario(data, column, title, ax):\n",
    "    # Create a target order for the comfort levels\n",
    "    target_order = [\"Very uncomfortable\", \"Uncomfortable\", \"Neither comfortable nor uncomfortable\", \n",
    "                    \"Comfortable\", \"Very comfortable\"]\n",
    "    \n",
    "    # Count the values for each comfort level, filling missing categories with 0\n",
    "    counts = data[column].value_counts().reindex(target_order, fill_value=0)\n",
    "    \n",
    "    # Create the bar plot on the given axis\n",
    "    sns.barplot(x=counts.index, y=counts.values, ax=ax, color=\"black\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticklabels(['VU', 'U', 'N', 'C', 'VC'], rotation=45)\n",
    "    ax.set_ylabel('Number of Responses')\n",
    "\n",
    "# Set up the figure and GridSpec for arranging plots\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "grid = GridSpec(1, 3, figure=fig)\n",
    "\n",
    "# Plot for Scenario Sam\n",
    "ax1 = fig.add_subplot(grid[0, 0])\n",
    "plot_filtered_scenario(data_filtered_sam, 'comfort_scenario_Sam', 'a. Sam', ax1)\n",
    "\n",
    "# Plot for Scenario Dan\n",
    "ax2 = fig.add_subplot(grid[0, 1])\n",
    "plot_filtered_scenario(data_filtered_dan, 'comfort_scenario_Dan', 'b. Dan', ax2)\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "# Plot for Scenario Rebecca\n",
    "ax3 = fig.add_subplot(grid[0, 2])\n",
    "plot_filtered_scenario(data_filtered_rebecca, 'comfort_scenario_Rebecca', 'c. Rebecca', ax3)\n",
    "ax3.set_ylabel('')\n",
    "\n",
    "# Set overall layout and title\n",
    "plt.tight_layout()\n",
    "fig.suptitle(\"Comfort with Consensual Scenarios\", fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.subplots_adjust(top=0.88)\n",
    "\n",
    "# Save the plot as a PDF\n",
    "plt.savefig(\"filtered_consent_scenarios.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebaf11-92e5-4ed5-90de-dc17e989d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redo plotting where consent is unclear\n",
    "\n",
    "# Filter out missing values in the 'unclear_scenario_G' column\n",
    "data_filtered_max = data[~data['unclear_scenario_G'].isna()]\n",
    "\n",
    "# Function to create bar plots for specific scenarios\n",
    "def plot_unclear_scenario(data, column, ax):\n",
    "    # Create a target order for the comfort levels\n",
    "    target_order = [\"Very uncomfortable\", \"Uncomfortable\", \"Neither comfortable nor uncomfortable\", \n",
    "                    \"Comfortable\", \"Very comfortable\"]\n",
    "    \n",
    "    # Count the values for each comfort level, filling missing categories with 0\n",
    "    counts = data[column].value_counts().reindex(target_order, fill_value=0)\n",
    "    \n",
    "    # Create the bar plot\n",
    "    sns.barplot(x=counts.index, y=counts.values, ax=ax, color=\"black\")\n",
    "    ax.set_xticklabels(['VU', 'U', 'N', 'C', 'VC'], rotation=45)\n",
    "    ax.set_ylabel('Number of Responses')\n",
    "\n",
    "# Set up the figure and GridSpec for arranging plots\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "grid = GridSpec(1, 2, figure=fig)\n",
    "\n",
    "# Plot for unclear scenario\n",
    "ax1 = fig.add_subplot(grid[0, 0])\n",
    "plot_unclear_scenario(data_filtered_max, 'unclear_scenario_G', ax1)\n",
    "ax1.set_title(\"Unclear Scenario\")\n",
    "ax1.set_xlabel(None)\n",
    "ax1.set_ylabel(\"Number of Responses\")\n",
    "\n",
    "# Plot for Scenario Rebecca (from previous work)\n",
    "ax2 = fig.add_subplot(grid[0, 1])\n",
    "plot_unclear_scenario(data_filtered_rebecca, 'comfort_scenario_Rebecca', ax2)\n",
    "ax2.set_title(\"Rebecca\")\n",
    "ax2.set_ylabel(None)\n",
    "\n",
    "# Set the overall layout and save the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.88)\n",
    "\n",
    "# Save the combined plot as a PDF\n",
    "plt.savefig(\"unclear_and_rebecca_scenarios.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b28be09-9e95-43b0-9aa8-f5f535760d29",
   "metadata": {},
   "source": [
    "To this point, the analysis has captured comfort and response patterns without distinguishing between control and treatment groups. Next, we will examine differences across groups to determine the effects of the two treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02f074-db4d-4aeb-ba3b-d85bb9ebf16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign participant group based on the conditions\n",
    "data['group_number'] = np.nan  # Add a new column 'group_number' initialized with NaN\n",
    "\n",
    "# Assign values to 'group_number' based on conditions\n",
    "data['group_number'] = np.where(data['info_group_start'].notna(), 'info', data['group_number'])\n",
    "data['group_number'] = np.where(data['pre_test_start'].notna(), 'pressure', data['group_number'])\n",
    "\n",
    "# For any remaining NaN values, assign 'control' group\n",
    "data['group_number'].fillna('control', inplace=True)\n",
    "\n",
    "# Verify the assignment\n",
    "print(data['group_number'].value_counts())\n",
    "\n",
    "# Convert end and start times to numeric by replacing commas with dots and converting to float\n",
    "columns_to_convert = ['info_group_end', 'info_group_start', 'pre_test_end', 'pre_test_start']\n",
    "for column in columns_to_convert:\n",
    "    data[column] = data[column].str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "# Verify the conversions\n",
    "print(data[['info_group_end', 'info_group_start', 'pre_test_end', 'pre_test_start']].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fe748-cd3b-4372-89cc-9470fafdaa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine comfort level overall (not by group)\n",
    "\n",
    "# Dictionary to replace comfort level strings with numeric values\n",
    "comfort_map = {\n",
    "    'Very uncomfortable': 1,\n",
    "    'Uncomfortable': 2,\n",
    "    'Neither comfortable nor uncomfortable': 3,\n",
    "    'Comfortable': 4,\n",
    "    'Very comfortable': 5\n",
    "}\n",
    "\n",
    "# List of columns to convert\n",
    "columns_to_convert = ['non_chris', 'comfort_scenario_Sam', 'comfort_scenario_Dan', \n",
    "                      'comfort_scenario_Rebecca', 'non_jes_disc', 'non_beth', 'unclear_scenario_G']\n",
    "\n",
    "# Replace comfort levels with numeric values across the specified columns\n",
    "for column in columns_to_convert:\n",
    "    data[column] = data[column].replace(comfort_map).astype(float)\n",
    "\n",
    "# Calculate average comfort levels for aggregate, consensual, and non-consensual scenarios\n",
    "data['agg_mean'] = data[['non_chris', 'comfort_scenario_Sam', 'comfort_scenario_Dan', \n",
    "                         'comfort_scenario_Rebecca', 'non_jes_disc', 'non_beth', \n",
    "                         'unclear_scenario_G']].mean(axis=1, skipna=True)\n",
    "\n",
    "data['con_mean'] = data[['comfort_scenario_Sam', 'comfort_scenario_Dan', 'comfort_scenario_Rebecca']].mean(axis=1, skipna=True)\n",
    "\n",
    "data['non_mean'] = data[['non_chris', 'non_jes_disc', 'non_beth']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Calculate median comfort levels\n",
    "data['agg_med'] = data[['non_chris', 'comfort_scenario_Sam', 'comfort_scenario_Dan', \n",
    "                        'comfort_scenario_Rebecca', 'non_jes_disc', 'non_beth', \n",
    "                        'unclear_scenario_G']].median(axis=1, skipna=True)\n",
    "\n",
    "data['con_med'] = data[['comfort_scenario_Sam', 'comfort_scenario_Dan', 'comfort_scenario_Rebecca']].median(axis=1, skipna=True)\n",
    "\n",
    "data['non_med'] = data[['non_chris', 'non_jes_disc', 'non_beth']].median(axis=1, skipna=True)\n",
    "\n",
    "# Prepare data for plotting the boxplot of average comfort levels\n",
    "avg_columns = ['agg_mean', 'con_mean', 'non_mean']\n",
    "avg_df = data[avg_columns]\n",
    "\n",
    "# Melt the data for easier plotting\n",
    "data_long = avg_df.melt(var_name='variable', value_name='value')\n",
    "\n",
    "# Map variable names to more readable labels\n",
    "data_long['variable'] = data_long['variable'].map({\n",
    "    'agg_mean': 'Aggregate',\n",
    "    'con_mean': 'Consensual',\n",
    "    'non_mean': 'Non-Consensual'\n",
    "})\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='variable', y='value', data=data_long)\n",
    "plt.title(\"Average Level of Comfort by Scenario Type\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Scenario Type\", fontsize=10)\n",
    "plt.ylabel(\"Average Level of Comfort\", fontsize=10)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.grid(False)  # Remove grid lines\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"average_comfort_boxplot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb44ff-16da-4b6c-b126-65438c948d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look into sample characteristics\n",
    "\n",
    "# Gender distribution\n",
    "gender_counts = data['gender_identity'].value_counts()\n",
    "print(\"Gender Distribution:\\n\", gender_counts)\n",
    "\n",
    "# Race distribution with percentage\n",
    "race_counts = data['ethnicity'].value_counts(normalize=True) * 100\n",
    "race_counts = race_counts.sort_values(ascending=False)\n",
    "print(\"Race Distribution:\\n\", race_counts)\n",
    "\n",
    "# Age distribution: Replacing \"24 or older\" with 24 and converting to numeric\n",
    "data['age'].replace(\"24 or older\", 24, inplace=True)\n",
    "data['age'] = pd.to_numeric(data['age'], errors='coerce')\n",
    "\n",
    "# Summary statistics for age\n",
    "mean_age = data['age'].mean()\n",
    "median_age = data['age'].median()\n",
    "sd_age = data['age'].std()\n",
    "min_age = data['age'].min()\n",
    "max_age = data['age'].max()\n",
    "\n",
    "print(f\"Mean age: {mean_age}\")\n",
    "print(f\"Median age: {median_age}\")\n",
    "print(f\"Standard deviation of age: {sd_age}\")\n",
    "print(f\"Minimum age: {min_age}\")\n",
    "print(f\"Maximum age: {max_age}\")\n",
    "\n",
    "# Religious affiliation distribution and summary statistics\n",
    "data['religious_affiliation'] = pd.to_numeric(data['religious_affiliation'], errors='coerce')\n",
    "\n",
    "mean_relig = data['religious_affiliation'].mean()\n",
    "median_relig = data['religious_affiliation'].median()\n",
    "sd_relig = data['religious_affiliation'].std()\n",
    "min_relig = data['religious_affiliation'].min()\n",
    "max_relig = data['religious_affiliation'].max()\n",
    "\n",
    "print(f\"Mean religious score: {mean_relig}\")\n",
    "print(f\"Median religious score: {median_relig}\")\n",
    "print(f\"Standard deviation of religious score: {sd_relig}\")\n",
    "print(f\"Minimum religious score: {min_relig}\")\n",
    "print(f\"Maximum religious score: {max_relig}\")\n",
    "\n",
    "# College completion distribution\n",
    "college_completed_counts = data['education_completed'].value_counts(normalize=True) * 100\n",
    "print(\"College Completed Distribution:\\n\", college_completed_counts)\n",
    "\n",
    "# College region distribution\n",
    "college_region_counts = data['education_region'].value_counts(normalize=True) * 100\n",
    "print(\"College Region Distribution:\\n\", college_region_counts)\n",
    "\n",
    "# Filter data for participants who completed the survey\n",
    "data_complete = data[data['Progress'] == 100]\n",
    "\n",
    "# Sexual orientation distribution\n",
    "sexuality_counts = data_complete['sexual_orientation'].value_counts(normalize=True) * 100\n",
    "print(\"Sexual Orientation Distribution:\\n\", sexuality_counts)\n",
    "\n",
    "# Distribution of missing values in 'sexual_orientation' column by gender\n",
    "na_sexuality = data_complete[data_complete['sexual_orientation'].isna()]\n",
    "na_sexuality_gender_counts = na_sexuality['gender_identity'].value_counts(normalize=True) * 100\n",
    "print(\"Missing Sexuality by Gender Distribution:\\n\", na_sexuality_gender_counts)\n",
    "\n",
    "# Assault history by gender\n",
    "assault_female = data[data['gender_identity'] == 'Cisgender female']['incident_report'].value_counts(normalize=True) * 100\n",
    "assault_male = data[data['gender_identity'] == 'Cisgender male']['incident_report'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Assault History (Female):\\n\", assault_female)\n",
    "print(\"Assault History (Male):\\n\", assault_male)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79600696-a7b0-43be-a3c0-d56d575c85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trends in responses re mastrubation frequency\n",
    "\n",
    "# Subset participants who did not report masturbation frequency\n",
    "data_no_mast = data[data['freq_masturbation'].isna()]\n",
    "\n",
    "# Proportion of each gender with no entry in 'freq_masturbation'\n",
    "prop_na_by_gender = data_no_mast['gender_identity'].value_counts(normalize=True) * 100\n",
    "print(\"Proportion of participants with no entry in 'freq_masturbation' by gender:\\n\", prop_na_by_gender)\n",
    "\n",
    "# Convert 'freq_masturbation' to numeric\n",
    "data['freq_masturbation'] = pd.to_numeric(data['freq_masturbation'], errors='coerce')\n",
    "\n",
    "# Calculate average frequency of masturbation by gender\n",
    "avg_freq_by_gender = data.groupby('gender_identity')['freq_masturbation'].mean()\n",
    "print(\"Average Frequency of Masturbation by Gender:\\n\", avg_freq_by_gender)\n",
    "\n",
    "# Subset data by gender\n",
    "data_female = data[data['gender_identity'] == 'Cisgender female'].copy()\n",
    "data_male = data[data['gender_identity'] == 'Cisgender male'].copy()\n",
    "\n",
    "# Remove rows with missing 'freq_masturbation' values\n",
    "data_male = data_male.dropna(subset=['freq_masturbation'])\n",
    "data_female = data_female.dropna(subset=['freq_masturbation'])\n",
    "\n",
    "# Calculate sample mean and standard error for males\n",
    "sample_mean_male = data_male['freq_masturbation'].mean()\n",
    "se_male = data_male['freq_masturbation'].std() / np.sqrt(len(data_male))\n",
    "\n",
    "# Calculate z-score and p-value for comparison with female sample\n",
    "sample_mean_female = data_female['freq_masturbation'].mean()\n",
    "z_score = (sample_mean_male - sample_mean_female) / se_male\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
    "\n",
    "print(f\"Z-Score: {z_score}, P-Value: {p_value}\")\n",
    "\n",
    "# Create 'in_relationship' column\n",
    "data['in_relationship'] = np.where(data['relationship_status'] == 'Yes', 1, \n",
    "                                   np.where(pd.isna(data['relationship_status']), np.nan, 0))\n",
    "\n",
    "# Calculate average frequency of masturbation by relationship status and gender\n",
    "avg_freq_by_rel_gender = data.groupby(['in_relationship', 'gender_identity'])['freq_masturbation'].mean()\n",
    "print(\"Average Frequency by Relationship Status and Gender:\\n\", avg_freq_by_rel_gender)\n",
    "\n",
    "# Group data by 'freq_porn' and calculate percentages\n",
    "freq_porn_distribution = data['freq_porn'].value_counts(normalize=True) * 100\n",
    "print(\"Porn Frequency Distribution:\\n\", freq_porn_distribution)\n",
    "\n",
    "# Remove rows with missing 'freq_porn'\n",
    "data_male = data_male.dropna(subset=['freq_porn'])\n",
    "data_female = data_female.dropna(subset=['freq_porn'])\n",
    "\n",
    "# Calculate sample mean and standard error for 'freq_porn' in males\n",
    "sample_mean_porn_male = data_male['freq_porn'].mean()\n",
    "se_porn_male = data_male['freq_porn'].std() / np.sqrt(len(data_male))\n",
    "\n",
    "# Calculate z-score and p-value for comparison with female sample\n",
    "sample_mean_porn_female = data_female['freq_porn'].mean()\n",
    "z_score_porn = (sample_mean_porn_male - sample_mean_porn_female) / se_porn_male\n",
    "p_value_porn = 2 * (1 - stats.norm.cdf(abs(z_score_porn)))\n",
    "\n",
    "print(f\"Z-Score (Porn): {z_score_porn}, P-Value: {p_value_porn}\")\n",
    "\n",
    "# Convert 'freq_sex' to numeric and calculate correlations\n",
    "data['freq_sex'] = pd.to_numeric(data['freq_sex'], errors='coerce')\n",
    "\n",
    "# Male correlation between 'freq_sex' and 'freq_masturbation'\n",
    "male_corr = data_male[['freq_sex', 'freq_masturbation']].dropna().corr().loc['freq_sex', 'freq_masturbation']\n",
    "print(f\"Male Correlation (freq_sex and freq_masturbation): {male_corr}\")\n",
    "\n",
    "# Female correlation between 'freq_sex' and 'freq_porn'\n",
    "female_corr = data_female[['freq_sex', 'freq_porn']].dropna().corr().loc['freq_sex', 'freq_porn']\n",
    "print(f\"Female Correlation (freq_sex and freq_porn): {female_corr}\")\n",
    "\n",
    "# Correlation between 'freq_porn' and 'freq_masturbation' across all participants\n",
    "data_complete_porn = data.dropna(subset=['freq_porn', 'freq_masturbation'])\n",
    "porn_mast_corr = data_complete_porn[['freq_porn', 'freq_masturbation']].corr().loc['freq_porn', 'freq_masturbation']\n",
    "print(f\"Correlation between 'freq_porn' and 'freq_masturbation': {porn_mast_corr}\")\n",
    "\n",
    "# Analyze 'mast_no_porn' column\n",
    "data_complete_mast_no_porn = data.dropna(subset=['mast_no_porn'])\n",
    "data_complete_mast_no_porn['mast_no_porn'] = pd.to_numeric(data_complete_mast_no_porn['mast_no_porn'], errors='coerce')\n",
    "print(f\"Summary of 'mast_no_porn':\\n\", data_complete_mast_no_porn['mast_no_porn'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46dc6f-3b69-456b-806a-2b0791eae68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine social variables around sexuality\n",
    "\n",
    "# Age when participants first viewed adult material\n",
    "age_viewed_distribution = data['age_first_exposure'].value_counts(normalize=True) * 100\n",
    "age_viewed_distribution = age_viewed_distribution.sort_values(ascending=False)\n",
    "print(\"Age Viewed Distribution:\\n\", age_viewed_distribution)\n",
    "\n",
    "# Convert 'speak_sex' to numeric and filter out rows with missing values\n",
    "data['speak_sex'] = pd.to_numeric(data['speak_sex'], errors='coerce')\n",
    "data_speak_sex = data.dropna(subset=['speak_sex'])\n",
    "\n",
    "# Group by 'speak_sex' and calculate percentages\n",
    "speak_sex_distribution = data_speak_sex['speak_sex'].value_counts(normalize=True) * 100\n",
    "print(\"Speak About Sex Distribution:\\n\", speak_sex_distribution)\n",
    "\n",
    "# Convert 'speak_porn' to numeric and filter out rows with missing values\n",
    "data['speak_porn'] = pd.to_numeric(data['speak_porn'], errors='coerce')\n",
    "data_speak_porn = data.dropna(subset=['speak_porn'])\n",
    "\n",
    "# Group by 'speak_porn' and calculate percentages\n",
    "speak_porn_distribution = data_speak_porn['speak_porn'].value_counts(normalize=True) * 100\n",
    "print(\"Speak About Porn Distribution:\\n\", speak_porn_distribution)\n",
    "\n",
    "# Convert 'porn_partner' to numeric and filter out rows with missing values\n",
    "data['porn_partner'] = pd.to_numeric(data['porn_partner'], errors='coerce')\n",
    "data_porn_partner = data.dropna(subset=['porn_partner'])\n",
    "\n",
    "# Group by 'porn_partner' and calculate percentages\n",
    "porn_partner_distribution = data_porn_partner['porn_partner'].value_counts(normalize=True) * 100\n",
    "print(\"Porn Partner Distribution:\\n\", porn_partner_distribution)\n",
    "\n",
    "# Subset data by gender for 'porn_partner'\n",
    "data_male_partner = data_porn_partner[data_porn_partner['gender_identity'] == 'Cisgender male']\n",
    "data_female_partner = data_porn_partner[data_porn_partner['gender_identity'] == 'Cisgender female']\n",
    "\n",
    "print(\"Summary of Porn Partner (Male):\\n\", data_male_partner['porn_partner'].describe())\n",
    "print(\"Summary of Porn Partner (Female):\\n\", data_female_partner['porn_partner'].describe())\n",
    "\n",
    "# Convert 'never_speak_mast' to numeric and filter out rows with missing values\n",
    "data['never_speak_mast'] = pd.to_numeric(data['never_speak_mast'], errors='coerce')\n",
    "data_never_speak_mast = data.dropna(subset=['never_speak_mast'])\n",
    "\n",
    "# Group by 'never_speak_mast' and calculate percentages\n",
    "never_speak_mast_distribution = data_never_speak_mast['never_speak_mast'].value_counts(normalize=True) * 100\n",
    "print(\"Never Speak About Masturbation Distribution:\\n\", never_speak_mast_distribution)\n",
    "\n",
    "# Convert 'educ_sex' to numeric and filter out rows with missing values\n",
    "data['educ_sex'] = pd.to_numeric(data['educ_sex'], errors='coerce')\n",
    "data_educ_sex = data.dropna(subset=['educ_sex'])\n",
    "\n",
    "# Group by 'educ_sex' and calculate percentages\n",
    "educ_sex_distribution = data_educ_sex['educ_sex'].value_counts(normalize=True) * 100\n",
    "print(\"Education About Sex Distribution:\\n\", educ_sex_distribution)\n",
    "\n",
    "# Subset data by gender for 'educ_sex'\n",
    "data_male_educ = data_educ_sex[data_educ_sex['gender_identity'] == 'Cisgender male']\n",
    "data_female_educ = data_educ_sex[data_educ_sex['gender_identity'] == 'Cisgender female']\n",
    "\n",
    "print(\"Summary of Education About Sex (Male):\\n\", data_male_educ['educ_sex'].describe())\n",
    "print(\"Summary of Education About Sex (Female):\\n\", data_female_educ['educ_sex'].describe())\n",
    "\n",
    "# Calculate sample mean and standard error for males in 'educ_sex'\n",
    "sample_mean_male_educ = data_male_educ['educ_sex'].mean()\n",
    "se_male_educ = data_male_educ['educ_sex'].std() / np.sqrt(len(data_male_educ))\n",
    "\n",
    "# Calculate the z-score and p-value for comparison with female sample\n",
    "sample_mean_female_educ = data_female_educ['educ_sex'].mean()\n",
    "z_score_educ = (sample_mean_male_educ - sample_mean_female_educ) / se_male_educ\n",
    "p_value_educ = 2 * (1 - stats.norm.cdf(abs(z_score_educ)))\n",
    "\n",
    "print(f\"Z-Score (Education About Sex): {z_score_educ}, P-Value: {p_value_educ}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b4937-695d-4c2d-988c-6418c6a577e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trends in finding pornography appealing\n",
    "\n",
    "# Convert 'appeals' to numeric and filter out rows with missing values\n",
    "data['appeals'] = pd.to_numeric(data['appeals'], errors='coerce')\n",
    "data_appeals = data.dropna(subset=['appeals'])\n",
    "\n",
    "# Group by 'appeals' and calculate percentages\n",
    "appeals_distribution = data_appeals['appeals'].value_counts(normalize=True) * 100\n",
    "print(\"Appeals Distribution:\\n\", appeals_distribution.sort_values(ascending=False))\n",
    "\n",
    "# Subset data by gender for 'appeals'\n",
    "data_male_appeals = data_appeals[data_appeals['gender_identity'] == 'Cisgender male']\n",
    "data_female_appeals = data_appeals[data_appeals['gender_identity'] == 'Cisgender female']\n",
    "\n",
    "# Summary statistics for 'appeals' by gender\n",
    "print(\"Summary of Appeals (Male):\\n\", data_male_appeals['appeals'].describe())\n",
    "print(\"Summary of Appeals (Female):\\n\", data_female_appeals['appeals'].describe())\n",
    "\n",
    "# Calculate sample mean and standard error for males in 'appeals'\n",
    "sample_mean_male_appeals = data_male_appeals['appeals'].mean()\n",
    "se_male_appeals = data_male_appeals['appeals'].std() / np.sqrt(len(data_male_appeals))\n",
    "\n",
    "# Calculate the z-score and p-value for comparison with female sample\n",
    "sample_mean_female_appeals = data_female_appeals['appeals'].mean()\n",
    "z_score_appeals = (sample_mean_male_appeals - sample_mean_female_appeals) / se_male_appeals\n",
    "p_value_appeals = 2 * (1 - stats.norm.cdf(abs(z_score_appeals)))\n",
    "\n",
    "print(f\"Z-Score (Appeals): {z_score_appeals}, P-Value: {p_value_appeals}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0c7cd-3469-4d7f-9e18-c9e5bc7f44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#knowledge and attitudes around pornography\n",
    "\n",
    "# Convert 'dont_know_porn_made' to numeric and filter out rows with missing values\n",
    "data['dont_know_porn_made'] = pd.to_numeric(data['dont_know_porn_made'], errors='coerce')\n",
    "data_dont_know = data.dropna(subset=['dont_know_porn_made'])\n",
    "\n",
    "# Group by 'dont_know_porn_made' and calculate percentages\n",
    "dont_know_distribution = data_dont_know['dont_know_porn_made'].value_counts(normalize=True) * 100\n",
    "print(\"Don't Know Porn Made Distribution:\\n\", dont_know_distribution)\n",
    "\n",
    "# Create a histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data_dont_know['dont_know_porn_made'], bins=10, kde=False)\n",
    "plt.title(\"Histogram of 'Don't Know Porn Made'\")\n",
    "plt.xlabel(\"Don't Know Porn Made\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Subset data by gender for 'dont_know_porn_made'\n",
    "data_male_dont_know = data_dont_know[data_dont_know['gender_identity'] == 'Cisgender male']\n",
    "data_female_dont_know = data_dont_know[data_dont_know['gender_identity'] == 'Cisgender female']\n",
    "\n",
    "print(\"Summary of Don't Know Porn Made (Male):\\n\", data_male_dont_know['dont_know_porn_made'].describe())\n",
    "print(\"Summary of Don't Know Porn Made (Female):\\n\", data_female_dont_know['dont_know_porn_made'].describe())\n",
    "\n",
    "# Calculate sample mean and standard error for males\n",
    "sample_mean_male_dont_know = data_male_dont_know['dont_know_porn_made'].mean()\n",
    "se_male_dont_know = data_male_dont_know['dont_know_porn_made'].std() / np.sqrt(len(data_male_dont_know))\n",
    "\n",
    "# Calculate the z-score and p-value for comparison with female sample\n",
    "sample_mean_female_dont_know = data_female_dont_know['dont_know_porn_made'].mean()\n",
    "z_score_dont_know = (sample_mean_male_dont_know - sample_mean_female_dont_know) / se_male_dont_know\n",
    "p_value_dont_know = 2 * (1 - stats.norm.cdf(abs(z_score_dont_know)))\n",
    "\n",
    "print(f\"Z-Score (Don't Know Porn Made): {z_score_dont_know}, P-Value: {p_value_dont_know}\")\n",
    "\n",
    "# Convert 'guilt_porn' to numeric and filter out rows with missing values\n",
    "data['guilt_feelings'] = pd.to_numeric(data['guilt_feelings'], errors='coerce')\n",
    "data_guilt = data.dropna(subset=['guilt_feelings'])\n",
    "\n",
    "# Group by 'guilt_porn' and calculate percentages\n",
    "guilt_distribution = data_guilt['guilt_feelings'].value_counts(normalize=True) * 100\n",
    "print(\"Guilt Feelings Distribution:\\n\", guilt_distribution)\n",
    "\n",
    "# Subset data by gender for 'guilt_feelings'\n",
    "data_male_guilt = data_guilt[data_guilt['gender_identity'] == 'Cisgender male']\n",
    "data_female_guilt = data_guilt[data_guilt['gender_identity'] == 'Cisgender female']\n",
    "\n",
    "print(\"Summary of Guilt Feelings (Male):\\n\", data_male_guilt['guilt_feelings'].describe())\n",
    "print(\"Summary of Guilt Feelings (Female):\\n\", data_female_guilt['guilt_feelings'].describe())\n",
    "\n",
    "# Calculate sample mean and standard error for males\n",
    "sample_mean_male_guilt = data_male_guilt['guilt_feelings'].mean()\n",
    "se_male_guilt = data_male_guilt['guilt_feelings'].std() / np.sqrt(len(data_male_guilt))\n",
    "\n",
    "# Calculate the z-score and p-value for comparison with female sample\n",
    "sample_mean_female_guilt = data_female_guilt['guilt_feelings'].mean()\n",
    "z_score_guilt = (sample_mean_male_guilt - sample_mean_female_guilt) / se_male_guilt\n",
    "p_value_guilt = 2 * (1 - stats.norm.cdf(abs(z_score_guilt)))\n",
    "\n",
    "print(f\"Z-Score (Guilt Feelings): {z_score_guilt}, P-Value: {p_value_guilt}\")\n",
    "\n",
    "# Convert 'uncomfy_speak_sex' to numeric and filter out rows with missing values\n",
    "data['uncomfy_speak_sex'] = pd.to_numeric(data['uncomfy_speak_sex'], errors='coerce')\n",
    "data_uncomfy_speak_sex = data.dropna(subset=['uncomfy_speak_sex'])\n",
    "\n",
    "# Group by 'uncomfy_speak_sex' and calculate percentages\n",
    "uncomfy_speak_sex_distribution = data_uncomfy_speak_sex['uncomfy_speak_sex'].value_counts(normalize=True) * 100\n",
    "print(\"Uncomfortable Speaking About Sex Distribution:\\n\", uncomfy_speak_sex_distribution)\n",
    "\n",
    "# Convert 'friends_okay' and 'ppl_okay' to numeric and filter out missing values\n",
    "data['friends_comfortable'] = pd.to_numeric(data['friends_comfortable'], errors='coerce')\n",
    "data['people_comfortable'] = pd.to_numeric(data['people_comfortable'], errors='coerce')\n",
    "\n",
    "data_friends_ppl_okay = data.dropna(subset=['friends_comfortable', 'people_comfortable'])\n",
    "\n",
    "# Group by 'friends_comfortable' and 'people_comfortable' and calculate percentages\n",
    "friends_distribution = data_friends_ppl_okay['friends_comfortable'].value_counts(normalize=True) * 100\n",
    "people_distribution = data_friends_ppl_okay['people_comfortable'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Friends Comfortable Distribution:\\n\", friends_distribution)\n",
    "print(\"People Comfortable Distribution:\\n\", people_distribution)\n",
    "\n",
    "# Correlation between 'friends_comfortable' and 'people_comfortable'\n",
    "correlation_friends_ppl_okay = data_friends_ppl_okay[['friends_comfortable', 'people_comfortable']].corr().loc['friends_comfortable', 'people_comfortable']\n",
    "print(f\"Correlation between Friends Comfortable and People Comfortable: {correlation_friends_ppl_okay}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15fde3-d9ee-407d-915b-4f61228c0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at survey-taking characteristics\n",
    "\n",
    "# Convert 'Duration_in_seconds' to numeric\n",
    "data['duration_seconds'] = pd.to_numeric(data['duration_seconds'], errors='coerce')\n",
    "\n",
    "# Filter the data for participants who finished the survey\n",
    "data_complete = data[data['survey_completed'] == True]\n",
    "\n",
    "# Remove outliers based on the 'duration_seconds' column (mean + 3 * std)\n",
    "mean_val = data_complete['duration_seconds'].mean()\n",
    "sd_val = data_complete['duration_seconds'].std()\n",
    "outlier_threshold = mean_val + 3 * sd_val\n",
    "\n",
    "# Filter data to exclude outliers\n",
    "data_no_outliers = data[data['duration_seconds'] <= outlier_threshold]\n",
    "\n",
    "# Summary statistics for 'duration_seconds' after removing outliers\n",
    "print(\"Summary of Duration in Seconds (No Outliers):\\n\", data_no_outliers['duration_seconds'].describe())\n",
    "\n",
    "# Analyze time spent by gender (female participants)\n",
    "data_female_complete = data_complete[data_complete['gender_identity'] == 'Cisgender female']\n",
    "\n",
    "mean_val_female = data_female_complete['duration_seconds'].mean()\n",
    "sd_val_female = data_female_complete['duration_seconds'].std()\n",
    "outlier_threshold_female = mean_val_female + 3 * sd_val_female\n",
    "\n",
    "# Filter data for females to exclude outliers\n",
    "data_female_no_outliers = data_female_complete[data_female_complete['duration_seconds'] <= outlier_threshold_female]\n",
    "\n",
    "# Summary statistics for 'duration_seconds' for females after removing outliers\n",
    "print(\"Summary of Duration in Seconds (Female, No Outliers):\\n\", data_female_no_outliers['duration_seconds'].describe())\n",
    "\n",
    "# Convert 'Progress' to numeric\n",
    "data['survey_progress'] = pd.to_numeric(data['survey_progress'], errors='coerce')\n",
    "\n",
    "# Analyze progress for female participants\n",
    "data_female_progress = data[data['gender_identity'] == 'Cisgender female']\n",
    "\n",
    "# Summary of 'survey_progress' for females\n",
    "print(\"Summary of Survey Progress (Female):\\n\", data_female_progress['survey_progress'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8f408-0b71-4fb8-9eaa-40cf0a578268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to convert to numeric\n",
    "columns_to_convert = [\n",
    "    'guilt_feelings', 'speak_sex', 'freq_porn', 'religious_affiliation', 'people_comfortable', 'friends_comfortable', \n",
    "    'incident_report', 'freq_masturbation', 'mast_no_activity', 'uncomfy_speak_sex', 'never_speak_activity', \n",
    "    'speak_porn', 'porn_partner', 'dont_know_creation', 'appeals', 'knowledge_creation', 'educ_sex', \n",
    "    'friends_comfortable', 'people_comfortable'\n",
    "]\n",
    "\n",
    "# Loop through the columns and convert them to numeric\n",
    "for column in columns_to_convert:\n",
    "    data[column] = pd.to_numeric(data[column], errors='coerce')\n",
    "\n",
    "# Clean up categorical variables using mapping where necessary\n",
    "# College completed\n",
    "data['education_completed'] = data['education_completed'].map({\n",
    "    'Less than one year': 0, '1 year': 1, '2 years': 2, '3 years': 3, '4 years': 4, '5+ years': 5\n",
    "}).astype(float)\n",
    "\n",
    "# Sexuality to numeric\n",
    "data['sexual_orientation'] = pd.to_numeric(data['sexual_orientation'], errors='coerce')\n",
    "\n",
    "# Age cleanup\n",
    "data['age'] = data['age'].replace({\n",
    "    '24 or older': 24, '23': 23, '22': 22, '21': 21, '20': 20, '19': 19, '18': 18\n",
    "}).astype(float)\n",
    "\n",
    "# Number of partners\n",
    "data['num_partners'] = data['num_partners'].map({\n",
    "    '0': 0, '1-2': 1, '3-4': 2, '5-6': 3, '6 or more': 4\n",
    "}).astype(float)\n",
    "\n",
    "# Assign numeric values for gender\n",
    "data['gender_identity'] = data['gender_identity'].map({\n",
    "    'Cisgender female': 0, 'Cisgender male': 1\n",
    "}).astype(float)\n",
    "\n",
    "# Alcohol consumption per week\n",
    "data['alc_per_week'] = data['alc_per_week'].map({\n",
    "    '0 times per week': 0, '1-2 times per week': 1, '3-4 times per week': 2, '5-6 times per week': 3, '7 or more times per week': 4\n",
    "}).astype(float)\n",
    "\n",
    "# Relationship status\n",
    "data['relationship_status'] = data['relationship_status'].map({\n",
    "    'Yes': 1, 'No': 0, 'Maybe': 0\n",
    "}).astype(float)\n",
    "\n",
    "# Frequency of sex\n",
    "data['freq_sex'] = pd.to_numeric(data['freq_sex'], errors='coerce')\n",
    "\n",
    "# Age first viewed adult material\n",
    "data['age_first_exposure'] = data['age_first_exposure'].map({\n",
    "    '9-11': 1, '12-13': 2, '14-15': 3, '16-17': 4, '18+': 5, 'N/A': 0\n",
    "}).astype(float)\n",
    "\n",
    "# Verify that columns were correctly converted\n",
    "print(data.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21680da2-d5b5-4cc9-8173-04a2518fd681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'freq_porn' is not NaN\n",
    "data_filtered = data.dropna(subset=['freq_porn'])\n",
    "\n",
    "# Create a binary 'watched_porn' variable based on 'freq_porn'\n",
    "data_filtered['watched_porn'] = np.where(data_filtered['freq_porn'].isin([0, 1]), 0, 1)\n",
    "\n",
    "# Create a binary 'race_white' variable based on race\n",
    "data_filtered['race_white'] = np.where(data_filtered['ethnicity'] == 'White / Caucasian', 1, 0)\n",
    "\n",
    "# Verify the new columns\n",
    "print(data_filtered[['watched_porn', 'race_white']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9f39b-c44a-4732-a4b0-970c704802d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifying htat my results align with the primary study that this project is based on\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the independent variables and dependent variable for the logistic regression model\n",
    "X = data_filtered[['gender_identity', 'age', 'race_white', 'religious_affiliation', 'education_completed', 'sexual_orientation', 'alc_per_week']]\n",
    "X = sm.add_constant(X)  # Add a constant term for the intercept\n",
    "y = data_filtered['watched_porn']\n",
    "\n",
    "# Fit the logistic regression model (equivalent to glm with binomial family in R)\n",
    "modelA = sm.Logit(y, X).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(modelA.summary())\n",
    "\n",
    "# Extract null deviance and degrees of freedom\n",
    "null_deviance = modelA.llnull\n",
    "df_null = modelA.df_model + 1  # Adding 1 for intercept\n",
    "\n",
    "# Number of observations\n",
    "n_obs = len(modelA.fittedvalues)\n",
    "print(f\"Null Deviance: {null_deviance}\")\n",
    "print(f\"Degrees of Freedom (Null): {df_null}\")\n",
    "print(f\"Number of Observations: {n_obs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c21581-2c2d-4616-b6de-450c20b9aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replicating model A from cooper and klein:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Prepare the independent variables and dependent variable for the logistic regression model\n",
    "X = data_filtered[['gender_identity', 'age', 'race_white', 'religious_affiliation', 'education_completed', 'sexual_orientation', 'alc_per_week']]\n",
    "X = sm.add_constant(X)  # Add a constant term for the intercept\n",
    "y = data_filtered['watched_porn']\n",
    "\n",
    "# Fit the logistic regression model\n",
    "modelA = sm.Logit(y, X).fit()\n",
    "\n",
    "# Extract coefficients and standard errors\n",
    "coefficients = modelA.params\n",
    "std_errors = modelA.bse\n",
    "\n",
    "# Calculate odds ratios using exp()\n",
    "odds_ratios = np.exp(coefficients)\n",
    "\n",
    "# Calculate standard errors of the odds ratios\n",
    "odds_ratios_std_errors = odds_ratios * std_errors\n",
    "\n",
    "# Extract p-values from the model\n",
    "p_values = modelA.pvalues\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "result = pd.DataFrame({\n",
    "    'Coefficient': coefficients,\n",
    "    'Std_Error': std_errors,\n",
    "    'Odds_Ratio': odds_ratios,\n",
    "    'Odds_Ratio_Std_Error': odds_ratios_std_errors,\n",
    "    'P_Value': p_values\n",
    "})\n",
    "\n",
    "# Calculate residuals and fitted values\n",
    "residuals = modelA.resid_response\n",
    "fitted_values = modelA.fittedvalues\n",
    "\n",
    "# Calculate chi-squared statistic\n",
    "chi_squared = np.sum((residuals ** 2) / (fitted_values * (1 - fitted_values)))\n",
    "df = len(coefficients) - 1\n",
    "nobs = len(residuals)\n",
    "\n",
    "# Add chi-squared, degrees of freedom, and number of observations to the results\n",
    "result['Chi_Squared'] = chi_squared\n",
    "result['Degrees_Freedom'] = df\n",
    "result['N_Obs'] = nobs\n",
    "\n",
    "# Display the result DataFrame\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810fcf3-8f30-47a5-9a99-c8c263f77eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now model B for keep and klein:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Prepare the independent variables and dependent variable for the logistic regression model (Model B)\n",
    "X_B = data_filtered[['gender_identity', 'age', 'race_white', 'religious_affiliation', 'education_completed', \n",
    "                     'sexual_orientation', 'alc_per_week', 'num_partners', 'relationship_status', 'freq_sex', \n",
    "                     'age_first_exposure', 'freq_masturbation']]\n",
    "X_B = sm.add_constant(X_B)  # Add a constant term for the intercept\n",
    "y_B = data_filtered['watched_porn']\n",
    "\n",
    "# Fit the logistic regression model (Model B)\n",
    "modelB = sm.Logit(y_B, X_B).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(modelB.summary())\n",
    "\n",
    "# Extract null deviance and degrees of freedom\n",
    "null_deviance_B = modelB.llnull\n",
    "df_null_B = modelB.df_model + 1  # Adding 1 for intercept\n",
    "\n",
    "# Number of observations\n",
    "n_obs_B = len(modelB.fittedvalues)\n",
    "print(f\"Null Deviance (Model B): {null_deviance_B}\")\n",
    "print(f\"Degrees of Freedom (Null, Model B): {df_null_B}\")\n",
    "print(f\"Number of Observations (Model B): {n_obs_B}\")\n",
    "\n",
    "# Extract coefficients, standard errors, and p-values\n",
    "coefficients_B = modelB.params\n",
    "std_errors_B = modelB.bse\n",
    "p_values_B = modelB.pvalues\n",
    "\n",
    "# Calculate odds ratios using exp()\n",
    "odds_ratios_B = np.exp(coefficients_B)\n",
    "\n",
    "# Calculate standard errors of the odds ratios\n",
    "odds_ratios_std_errors_B = odds_ratios_B * std_errors_B\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "result_B = pd.DataFrame({\n",
    "    'Coefficient': coefficients_B,\n",
    "    'Std_Error': std_errors_B,\n",
    "    'Odds_Ratio': odds_ratios_B,\n",
    "    'Odds_Ratio_Std_Error': odds_ratios_std_errors_B,\n",
    "    'P_Value': p_values_B\n",
    "})\n",
    "\n",
    "# Calculate residuals and fitted values\n",
    "residuals_B = modelB.resid_response\n",
    "fitted_values_B = modelB.fittedvalues\n",
    "\n",
    "# Calculate chi-squared statistic\n",
    "chi_squared_B = np.sum((residuals_B ** 2) / (fitted_values_B * (1 - fitted_values_B)))\n",
    "df_B = len(coefficients_B) - 1\n",
    "nobs_B = len(residuals_B)\n",
    "\n",
    "# Add chi-squared, degrees of freedom, and number of observations to the results\n",
    "result_B['Chi_Squared'] = chi_squared_B\n",
    "result_B['Degrees_Freedom'] = df_B\n",
    "result_B['N_Obs'] = nobs_B\n",
    "\n",
    "# Display the result DataFrame\n",
    "print(result_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46c332-5d9f-438b-a9cf-e043ff080467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now model C from Cooper and Klein\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Prepare the independent variables and dependent variable for the logistic regression model (Model C)\n",
    "X_C = data_filtered[['gender_identity', 'age', 'race_white', 'religious_affiliation', 'education_completed', \n",
    "                     'sexual_orientation', 'alc_per_week', 'num_partners', 'relationship_status', 'freq_sex', \n",
    "                     'age_first_exposure', 'freq_masturbation', 'speak_porn', 'speak_sex', \n",
    "                     'friends_comfortable', 'people_comfortable']]\n",
    "X_C = sm.add_constant(X_C)  # Add a constant term for the intercept\n",
    "y_C = data_filtered['watched_porn']\n",
    "\n",
    "# Fit the logistic regression model (Model C)\n",
    "modelC = sm.Logit(y_C, X_C).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(modelC.summary())\n",
    "\n",
    "# Extract null deviance and degrees of freedom\n",
    "null_deviance_C = modelC.llnull\n",
    "df_null_C = modelC.df_model + 1  # Adding 1 for intercept\n",
    "\n",
    "# Number of observations\n",
    "n_obs_C = len(modelC.fittedvalues)\n",
    "print(f\"Null Deviance (Model C): {null_deviance_C}\")\n",
    "print(f\"Degrees of Freedom (Null, Model C): {df_null_C}\")\n",
    "print(f\"Number of Observations (Model C): {n_obs_C}\")\n",
    "\n",
    "# Extract coefficients, standard errors, and p-values\n",
    "coefficients_C = modelC.params\n",
    "std_errors_C = modelC.bse\n",
    "p_values_C = modelC.pvalues\n",
    "\n",
    "# Calculate odds ratios using exp()\n",
    "odds_ratios_C = np.exp(coefficients_C)\n",
    "\n",
    "# Calculate standard errors of the odds ratios\n",
    "odds_ratios_std_errors_C = odds_ratios_C * std_errors_C\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "result_C = pd.DataFrame({\n",
    "    'Coefficient': coefficients_C,\n",
    "    'Std_Error': std_errors_C,\n",
    "    'Odds_Ratio': odds_ratios_C,\n",
    "    'Odds_Ratio_Std_Error': odds_ratios_std_errors_C,\n",
    "    'P_Value': p_values_C\n",
    "})\n",
    "\n",
    "# Calculate residuals and fitted values\n",
    "residuals_C = modelC.resid_response\n",
    "fitted_values_C = modelC.fittedvalues\n",
    "\n",
    "# Calculate chi-squared statistic\n",
    "chi_squared_C = np.sum((residuals_C ** 2) / (fitted_values_C * (1 - fitted_values_C)))\n",
    "df_C = len(coefficients_C) - 1\n",
    "nobs_C = len(residuals_C)\n",
    "\n",
    "# Add chi-squared, degrees of freedom, and number of observations to the results\n",
    "result_C['Chi_Squared'] = chi_squared_C\n",
    "result_C['Degrees_Freedom'] = df_C\n",
    "result_C['N_Obs'] = nobs_C\n",
    "\n",
    "# Display the result DataFrame\n",
    "print(result_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f767a-866f-4324-af9d-3fc10bde8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#did men often stop taking the survey after reading the vignettes? (sign of discomfort)\n",
    "\n",
    "# Get the index of the 'unclear_scenario_G' column\n",
    "unc_max_col = data.columns.get_loc('unclear_scenario_G')\n",
    "\n",
    "# Select columns from 'unclear_scenario_G' to the end\n",
    "selected_cols = ['gender_identity'] + data.columns[unc_max_col + 1:].tolist()\n",
    "\n",
    "# Subset the data frame with the selected columns\n",
    "data_filtered = data[selected_cols]\n",
    "\n",
    "# Split the data into male and female subsets\n",
    "female_df = data_filtered[data_filtered['gender_identity'] == 0]\n",
    "male_df = data_filtered[data_filtered['gender_identity'] == 1]\n",
    "\n",
    "# Calculate the number of missing values in each row for males and females\n",
    "female_na_count = female_df.isna().sum(axis=1)\n",
    "male_na_count = male_df.isna().sum(axis=1)\n",
    "\n",
    "# Calculate and print the mean number of missing values for each gender\n",
    "mean_female_na = female_na_count.mean()\n",
    "mean_male_na = male_na_count.mean()\n",
    "\n",
    "print(f\"Mean number of missing values (Female): {mean_female_na}\")\n",
    "print(f\"Mean number of missing values (Male): {mean_male_na}\")\n",
    "\n",
    "#now same Q, but by group\n",
    "# Subset the dataset to only include males\n",
    "males = data_filtered[data_filtered['gender_identity'] == 1]\n",
    "\n",
    "# Subset the male dataset to only include \"info\" and \"control\" groups\n",
    "male_info_control = males[males['group_number'].isin(['info', 'control'])]\n",
    "\n",
    "# Create a new column called \"num_nas\" that counts the number of NAs in each row\n",
    "male_info_control['num_nas'] = male_info_control.isna().sum(axis=1)\n",
    "\n",
    "# Calculate the mean number of NAs for males in the \"info\" group and \"control\" group\n",
    "mean_nas_info = male_info_control.loc[male_info_control['group_number'] == 'info', 'num_nas'].mean()\n",
    "mean_nas_control = male_info_control.loc[male_info_control['group_number'] == 'control', 'num_nas'].mean()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean number of NAs for males in the 'info' group: {mean_nas_info}\")\n",
    "print(f\"Mean number of NAs for males in the 'control' group: {mean_nas_control}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5745bcf-bca7-4f3f-abeb-50c2c0fe2a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at trends in time spent on conditions - check to see if they actually read it, and separating out effects for those who did read it\n",
    "\n",
    "# Create new columns for the amount of time spent reading the condition\n",
    "data['control_time_spent'] = np.nan  # Create a column but leave it empty for now\n",
    "data['info_time_spent'] = data['info_group_end'] - data['info_group_start']\n",
    "data['pre_time_spent'] = data['pre_test_end'] - data['pre_test_start']\n",
    "\n",
    "# Calculate mean of control_time_spent for group \"control\"\n",
    "mean_control = data.groupby('group_number')['control_time_spent'].mean().loc['control']\n",
    "\n",
    "# Calculate mean of info_time_spent for group \"info\"\n",
    "mean_info = data.groupby('group_number')['info_time_spent'].mean().loc['info']\n",
    "\n",
    "# Calculate mean of pre_time_spent for group \"pressure\"\n",
    "mean_pressure = data.groupby('group_number')['pre_time_spent'].mean().loc['pressure']\n",
    "\n",
    "# Filter data to include only female participants\n",
    "data_female = data[data['gender_identity'] == 0]\n",
    "\n",
    "# Convert 'info_feels_pos' and 'pre_feels_pos' to numeric\n",
    "data['info_feels_pos'] = pd.to_numeric(data['info_feels_pos'], errors='coerce')\n",
    "data['pre_feels_pos'] = pd.to_numeric(data['pre_feels_pos'], errors='coerce')\n",
    "\n",
    "# Summary statistics for 'info_feels_pos'\n",
    "print(\"Summary of 'info_feels_pos':\\n\", data['info_feels_pos'].describe())\n",
    "\n",
    "# Remove rows where less than a certain amount of time was spent on specific conditions\n",
    "data_filtered = data[~((data['group_number'] == 'pressure') & (data['pre_time_spent'] < 30))]\n",
    "data_filtered = data_filtered[~((data['group_number'] == 'info') & (data['info_time_spent'] < 20))]\n",
    "\n",
    "# Verify the filtering\n",
    "print(f\"Rows remaining after filtering: {len(data_filtered)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f692c-bcbc-45e6-8841-b48de4f7a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r did men stop taking the survey after porn vignettes (BY group)\n",
    "\n",
    "# Get the index of the 'unclear_scenario_G' column\n",
    "unc_max_col = data.columns.get_loc('unclear_scenario_G')\n",
    "\n",
    "# Select columns from 'unclear_scenario_G' to the end, including 'gender_identity'\n",
    "selected_cols = ['gender_identity'] + data.columns[unc_max_col + 1:].tolist()\n",
    "\n",
    "# Subset the data with the selected columns\n",
    "data_filtered = data[selected_cols]\n",
    "\n",
    "# Subset the dataset to only include males (gender == 0)\n",
    "males = data_filtered[data_filtered['gender_identity'] == 0]\n",
    "\n",
    "# Subset the male dataset to only include \"info\" and \"control\" groups\n",
    "male_info_control = males[males['group_number'].isin(['info', 'control'])]\n",
    "\n",
    "# Create a new column called \"num_nas\" that counts the number of NAs in each row\n",
    "male_info_control['num_nas'] = male_info_control.isna().sum(axis=1)\n",
    "\n",
    "# Calculate the mean number of NAs for males in the \"info\" group and \"control\" group\n",
    "mean_nas_info = male_info_control.loc[male_info_control['group_number'] == 'info', 'num_nas'].mean()\n",
    "mean_nas_control = male_info_control.loc[male_info_control['group_number'] == 'control', 'num_nas'].mean()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean number of NAs for males in the 'info' group: {mean_nas_info}\")\n",
    "print(f\"Mean number of NAs for males in the 'control' group: {mean_nas_control}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4cf967-5aec-4e86-9bf6-e1bddb1d8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#figures in final report\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Set up the theme for plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create new treatment variable and map conditions\n",
    "data['treatment'] = data['group_number'].replace({'info': 'treatment', 'pressure': 'treatment', 'control': 'control'})\n",
    "\n",
    "# Melt the data for easy plotting\n",
    "avgs = ['treatment', 'agg_mean', 'con_mean', 'non_mean', 'unclear_scenario_G']\n",
    "avg_df = data[avgs]\n",
    "data_long = avg_df.melt(id_vars='treatment', var_name='variable', value_name='value')\n",
    "\n",
    "# Update variable names for better readability\n",
    "data_long['variable'] = data_long['variable'].replace({\n",
    "    'agg_mean': 'Aggregate', \n",
    "    'con_mean': 'Consensual', \n",
    "    'non_mean': 'Non-Consensual', \n",
    "    'unclear_scenario_G': 'Unclear'\n",
    "})\n",
    "\n",
    "# Drop rows with missing values\n",
    "data_long = data_long.dropna(subset=['value'])\n",
    "\n",
    "# Summary statistics and standard errors for plotting\n",
    "data_long_summary = data_long.groupby(['treatment', 'variable']).agg(\n",
    "    mean_value=('value', 'mean'),\n",
    "    se=('value', lambda x: np.std(x, ddof=1) / np.sqrt(len(x)))\n",
    ").reset_index()\n",
    "\n",
    "# Plotting: Boxplot for average comfort by scenario type, split by treatment\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='variable', y='value', hue='treatment', data=data_long, palette='muted')\n",
    "plt.title('Level of Comfort with Scenarios for all Participants')\n",
    "plt.xlabel('Scenario Type')\n",
    "plt.ylabel('Average Level of Comfort')\n",
    "plt.show()\n",
    "\n",
    "# Bar plot with error bars for mean values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='variable', y='mean_value', hue='treatment', data=data_long_summary, \n",
    "            palette={'info': '#9DC3E6', 'pressure': '#FAC8CD', 'control': '#CCF0E6'}, dodge=True, ci=None)\n",
    "\n",
    "# Add error bars\n",
    "for idx, row in data_long_summary.iterrows():\n",
    "    plt.errorbar(x=idx % len(data_long_summary['variable'].unique()), y=row['mean_value'], \n",
    "                 yerr=row['se'], fmt='none', color='black', capsize=5)\n",
    "\n",
    "plt.title('Average Level of Comfort by Treatment and Scenario Type')\n",
    "plt.xlabel('Scenario Type')\n",
    "plt.ylabel('Average Level of Comfort')\n",
    "plt.show()\n",
    "\n",
    "# Create plots for male and female participants separately\n",
    "for gender, gender_name in [(0, 'Female'), (1, 'Male')]:\n",
    "    gender_data = data[data['gender_identity'] == gender]\n",
    "    avg_df_gender = gender_data[['treatment', 'agg_mean', 'con_mean', 'non_mean', 'unclear_scenario_G']]\n",
    "    data_long_gender = avg_df_gender.melt(id_vars='treatment', var_name='variable', value_name='value')\n",
    "    data_long_gender['variable'] = data_long_gender['variable'].replace({\n",
    "        'agg_mean': 'Aggregate', \n",
    "        'con_mean': 'Consensual', \n",
    "        'non_mean': 'Non-Consensual', \n",
    "        'unclear_scenario_G': 'Unclear'\n",
    "    })\n",
    "    data_long_gender_summary = data_long_gender.groupby(['treatment', 'variable']).agg(\n",
    "        mean_value=('value', 'mean'),\n",
    "        se=('value', lambda x: np.std(x, ddof=1) / np.sqrt(len(x)))\n",
    "    ).reset_index()\n",
    "\n",
    "    # Bar plot for each gender\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='variable', y='mean_value', hue='treatment', data=data_long_gender_summary, dodge=True, \n",
    "                palette={'info': '#9DC3E6', 'pressure': '#FAC8CD', 'control': '#CCF0E6'})\n",
    "    \n",
    "    # Add error bars\n",
    "    for idx, row in data_long_gender_summary.iterrows():\n",
    "        plt.errorbar(x=idx % len(data_long_gender_summary['variable'].unique()), y=row['mean_value'], \n",
    "                     yerr=row['se'], fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    plt.title(f'Average Level of Comfort by Scenario Type - {gender_name}')\n",
    "    plt.xlabel('Scenario Type')\n",
    "    plt.ylabel('Average Level of Comfort')\n",
    "    plt.show()\n",
    "\n",
    "# Overall plot for male and female participants for all groups\n",
    "data_long_summary['treatment'] = data_long_summary['treatment'].replace({0: 'female', 1: 'male'})\n",
    "data_long_summary = data_long_summary.dropna(subset=['treatment'])\n",
    "\n",
    "# Plot for male and female participants\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='variable', y='mean_value', hue='treatment', data=data_long_summary, dodge=True, \n",
    "            palette={'female': '#FAC8CD', 'male': '#9DC3E6'})\n",
    "plt.title('Average Level of Comfort by Gender')\n",
    "plt.xlabel('Scenario Type')\n",
    "plt.ylabel('Average Level of Comfort')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1e256-0b6f-4ec0-b5a6-b533d85f7a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more plots in main file\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Function to prepare data for plotting\n",
    "def prepare_data(data, group_filter):\n",
    "    # Subset the data by group (control, info, pressure)\n",
    "    data_group = data[data['group_number'] == group_filter].copy()\n",
    "\n",
    "    # Map gender to treatment\n",
    "    data_group['treatment'] = data_group['gender_identity'].replace({0: 'female', 1: 'male'})\n",
    "\n",
    "    # Melt the data for easy plotting\n",
    "    avgs = ['treatment', 'con_mean', 'non_mean', 'unclear_scenario_G']\n",
    "    avg_df = data_group[avgs]\n",
    "    data_long = avg_df.melt(id_vars='treatment', var_name='variable', value_name='value')\n",
    "\n",
    "    # Rename the variable for clarity\n",
    "    data_long['variable'] = data_long['variable'].replace({\n",
    "        'con_mean': 'Consensual',\n",
    "        'non_mean': 'Non-Consensual',\n",
    "        'unclear_scenario_G': 'Unclear'\n",
    "    })\n",
    "\n",
    "    # Group by treatment and variable, calculate mean and standard error\n",
    "    data_long_summary = data_long.groupby(['treatment', 'variable']).agg(\n",
    "        mean_value=('value', 'mean'),\n",
    "        se=('value', lambda x: np.std(x, ddof=1) / np.sqrt(len(x)))\n",
    "    ).reset_index()\n",
    "\n",
    "    return data_long_summary\n",
    "\n",
    "# Prepare data for the \"control\", \"info\", and \"pressure\" groups\n",
    "data_control = prepare_data(data, 'control')\n",
    "data_info = prepare_data(data, 'info')\n",
    "data_pressure = prepare_data(data, 'pressure')\n",
    "\n",
    "# Function to create bar plots with error bars\n",
    "def plot_bar(data, title, ax):\n",
    "    sns.barplot(x='variable', y='mean_value', hue='treatment', data=data, dodge=True, palette={'female': '#FAC8CD', 'male': '#9DC3E6'}, ax=ax)\n",
    "    \n",
    "    # Add error bars\n",
    "    for idx, row in data.iterrows():\n",
    "        ax.errorbar(x=idx % len(data['variable'].unique()), y=row['mean_value'], \n",
    "                     yerr=row['se'], fmt='none', color='black', capsize=5)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Scenario Type')\n",
    "    ax.set_ylabel('Mean Comfort')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Set up the grid for plotting the three groups together\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot for \"control\" group\n",
    "plot_bar(data_control, 'a. Control', axes[0])\n",
    "\n",
    "# Plot for \"info\" group\n",
    "plot_bar(data_info, 'b. Information', axes[1])\n",
    "\n",
    "# Plot for \"pressure\" group\n",
    "plot_bar(data_pressure, 'c. Social Pressure', axes[2])\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f179bd2-b6d0-4dcf-9b01-a498be78b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression analysis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# 1. Recode gender and combine cis and trans\n",
    "data['gender_identity'] = data['gender_identity'].replace({0: 'female', 1: 'male'})\n",
    "\n",
    "# 2. Linear regression models with interaction terms\n",
    "# Define a function to run and summarize regression models\n",
    "def run_regression(formula, data):\n",
    "    model = ols(formula, data=data).fit()\n",
    "    return model.summary()\n",
    "\n",
    "# Model 1: Aggregate mean regression\n",
    "agg_model = run_regression('agg_mean ~ treatment + guilt_feelings + treatment*guilt_feelings + people_comfortable + friends_comfortable + freq_porn + speak_sex + religious_affiliation + gender_identity + treatment*gender_identity', data)\n",
    "print(agg_model)\n",
    "\n",
    "# Model 2: Consensual mean regression\n",
    "con_model = run_regression('con_mean ~ treatment + guilt_feelings + treatment*guilt_feelings + people_comfortable + friends_comfortable + freq_porn + speak_sex + religious_affiliation + gender_identity + treatment*gender_identity', data)\n",
    "print(con_model)\n",
    "\n",
    "# Model 3: Non-consensual mean regression\n",
    "non_model = run_regression('non_mean ~ treatment + guilt_feelings + treatment*guilt_feelings + people_comfortable + friends_comfortable + freq_porn + speak_sex + religious_affiliation + gender_identity + treatment*gender_identity', data)\n",
    "print(non_model)\n",
    "\n",
    "# Model 4: Unclear mean regression\n",
    "unclear_model = run_regression('unclear_scenario_G ~ treatment + guilt_feelings + treatment*guilt_feelings + people_comfortable + friends_comfortable + freq_porn + speak_sex + religious_affiliation + gender_identity + treatment*gender_identity', data)\n",
    "print(unclear_model)\n",
    "\n",
    "# 3. Run regression models for low comfort participants\n",
    "data_low = data[data['agg_mean'] <= 3]\n",
    "\n",
    "agg_low_model = run_regression('agg_mean ~ treatment + guilt_feelings + treatment*guilt_feelings + people_comfortable + friends_comfortable + freq_porn + speak_sex + religious_affiliation + gender_identity + treatment*gender_identity', data_low)\n",
    "print(agg_low_model)\n",
    "\n",
    "# Run regression models for high comfort participants\n",
    "data_high = data[data['agg_mean'] >= 3]\n",
    "\n",
    "agg_high_model = run_regression('agg_mean ~ treatment + guilt_feelings + treatment*guilt_feelings + people_comfortable + friends_comfortable + freq_porn + speak_sex + religious_affiliation + gender_identity + treatment*gender_identity', data_high)\n",
    "print(agg_high_model)\n",
    "\n",
    "# 4. Mean comparisons between men and women\n",
    "# Subset the data based on group_number\n",
    "data_control = data[data['group_number'] == \"control\"]\n",
    "data_info = data[data['group_number'] == \"info\"]\n",
    "data_pressure = data[data['group_number'] == \"pressure\"]\n",
    "\n",
    "# Calculate mean for agg_mean across groups (control, info, pressure)\n",
    "mean_control = data_control['agg_mean'].mean()\n",
    "mean_info = data_info['agg_mean'].mean()\n",
    "mean_pressure = data_pressure['agg_mean'].mean()\n",
    "\n",
    "print(f\"Mean (Control): {mean_control}\")\n",
    "print(f\"Mean (Info): {mean_info}\")\n",
    "print(f\"Mean (Pressure): {mean_pressure}\")\n",
    "\n",
    "# Calculate sample mean, standard error, z-score, and p-value for 'info' group\n",
    "se_info = data_info['agg_mean'].std() / np.sqrt(len(data_info))\n",
    "z_score_info = (mean_info - mean_control) / se_info\n",
    "p_value_info = 2 * (1 - stats.norm.cdf(abs(z_score_info)))\n",
    "\n",
    "print(f\"Z-Score (Info vs Control): {z_score_info}, P-Value: {p_value_info}\")\n",
    "\n",
    "# Calculate sample mean, standard error, z-score, and p-value for 'pressure' group\n",
    "se_pressure = data_pressure['agg_mean'].std() / np.sqrt(len(data_pressure))\n",
    "z_score_pressure = (mean_pressure - mean_control) / se_pressure\n",
    "p_value_pressure = 2 * (1 - stats.norm.cdf(abs(z_score_pressure)))\n",
    "\n",
    "print(f\"Z-Score (Pressure vs Control): {z_score_pressure}, P-Value: {p_value_pressure}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e86fd6-8244-4347-92c1-f6e02fe90e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing significance of both treatments for men\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Subset the data for males by group number (control, info, pressure)\n",
    "data_male_control = data_male[data_male['group_number'] == 'control']\n",
    "data_male_info = data_male[data_male['group_number'] == 'info']\n",
    "data_male_pressure = data_male[data_male['group_number'] == 'pressure']\n",
    "\n",
    "# Filter out rows with missing values for agg_mean\n",
    "data_male_control_agg = data_male_control.dropna(subset=['agg_mean'])\n",
    "data_male_info_agg = data_male_info.dropna(subset=['agg_mean'])\n",
    "data_male_pressure_agg = data_male_pressure.dropna(subset=['agg_mean'])\n",
    "\n",
    "# Print the means of agg_mean for control, info, and pressure groups\n",
    "print(f\"Mean (Control, agg_mean): {data_male_control_agg['agg_mean'].mean()}\")\n",
    "print(f\"Mean (Info, agg_mean): {data_male_info_agg['agg_mean'].mean()}\")\n",
    "print(f\"Mean (Pressure, agg_mean): {data_male_pressure_agg['agg_mean'].mean()}\")\n",
    "\n",
    "# Z-test: Compare agg_mean between info and control groups\n",
    "sample_mean_info = data_male_info_agg['agg_mean'].mean()\n",
    "se_info = data_male_info_agg['agg_mean'].std() / np.sqrt(len(data_male_info_agg))\n",
    "\n",
    "z_score_info = (sample_mean_info - data_male_control_agg['agg_mean'].mean()) / se_info\n",
    "p_value_info = 2 * (1 - stats.norm.cdf(abs(z_score_info)))\n",
    "\n",
    "print(f\"Z-Score (Info vs Control, agg_mean): {z_score_info}, P-Value: {p_value_info}\")\n",
    "\n",
    "# Z-test: Compare agg_mean between pressure and control groups\n",
    "sample_mean_pressure = data_male_pressure_agg['agg_mean'].mean()\n",
    "se_pressure = data_male_pressure_agg['agg_mean'].std() / np.sqrt(len(data_male_pressure_agg))\n",
    "\n",
    "z_score_pressure = (sample_mean_pressure - data_male_control_agg['agg_mean'].mean()) / se_pressure\n",
    "p_value_pressure = 2 * (1 - stats.norm.cdf(abs(z_score_pressure)))\n",
    "\n",
    "print(f\"Z-Score (Pressure vs Control, agg_mean): {z_score_pressure}, P-Value: {p_value_pressure}\")\n",
    "\n",
    "# Subset the data for con_mean\n",
    "data_male_control_con = data_male_control.dropna(subset=['con_mean'])\n",
    "data_male_info_con = data_male_info.dropna(subset=['con_mean'])\n",
    "data_male_pressure_con = data_male_pressure.dropna(subset=['con_mean'])\n",
    "\n",
    "# Print the means of con_mean for control, info, and pressure groups\n",
    "print(f\"Mean (Control, con_mean): {data_male_control_con['con_mean'].mean()}\")\n",
    "print(f\"Mean (Info, con_mean): {data_male_info_con['con_mean'].mean()}\")\n",
    "print(f\"Mean (Pressure, con_mean): {data_male_pressure_con['con_mean'].mean()}\")\n",
    "\n",
    "# Z-test: Compare con_mean between info and control groups\n",
    "sample_mean_con_info = data_male_info_con['con_mean'].mean()\n",
    "se_con_info = data_male_info_con['con_mean'].std() / np.sqrt(len(data_male_info_con))\n",
    "\n",
    "z_score_con_info = (sample_mean_con_info - data_male_control_con['con_mean'].mean()) / se_con_info\n",
    "p_value_con_info = 2 * (1 - stats.norm.cdf(abs(z_score_con_info)))\n",
    "\n",
    "print(f\"Z-Score (Info vs Control, con_mean): {z_score_con_info}, P-Value: {p_value_con_info}\")\n",
    "\n",
    "# Z-test: Compare con_mean between pressure and control groups\n",
    "sample_mean_con_pressure = data_male_pressure_con['con_mean'].mean()\n",
    "se_con_pressure = data_male_pressure_con['con_mean'].std() / np.sqrt(len(data_male_pressure_con))\n",
    "\n",
    "z_score_con_pressure = (sample_mean_con_pressure - data_male_control_con['con_mean'].mean()) / se_con_pressure\n",
    "p_value_con_pressure = 2 * (1 - stats.norm.cdf(abs(z_score_con_pressure)))\n",
    "\n",
    "print(f\"Z-Score (Pressure vs Control, con_mean): {z_score_con_pressure}, P-Value: {p_value_con_pressure}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ae3b8-6578-4165-a8c6-2d87749030c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the same stats, but only non-consensual\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Subset the data for males by group number (control, info, pressure)\n",
    "data_male_control = data_male[data_male['group_number'] == 'control']\n",
    "data_male_info = data_male[data_male['group_number'] == 'info']\n",
    "data_male_pressure = data_male[data_male['group_number'] == 'pressure']\n",
    "\n",
    "# Filter out rows with missing values for non_mean\n",
    "data_male_control_non_mean = data_male_control.dropna(subset=['non_mean'])\n",
    "data_male_info_non_mean = data_male_info.dropna(subset=['non_mean'])\n",
    "data_male_pressure_non_mean = data_male_pressure.dropna(subset=['non_mean'])\n",
    "\n",
    "# Print the means of non_mean for control, info, and pressure groups\n",
    "print(f\"Mean (Control, non_mean): {data_male_control_non_mean['non_mean'].mean()}\")\n",
    "print(f\"Mean (Info, non_mean): {data_male_info_non_mean['non_mean'].mean()}\")\n",
    "print(f\"Mean (Pressure, non_mean): {data_male_pressure_non_mean['non_mean'].mean()}\")\n",
    "\n",
    "# Z-test: Compare non_mean between info and control groups\n",
    "sample_mean_info = data_male_info_non_mean['non_mean'].mean()\n",
    "se_info = data_male_info_non_mean['non_mean'].std() / np.sqrt(len(data_male_info_non_mean))\n",
    "\n",
    "z_score_info = (sample_mean_info - data_male_control_non_mean['non_mean'].mean()) / se_info\n",
    "p_value_info = 2 * (1 - stats.norm.cdf(abs(z_score_info)))\n",
    "\n",
    "print(f\"Z-Score (Info vs Control, non_mean): {z_score_info}, P-Value: {p_value_info}\")\n",
    "\n",
    "# Z-test: Compare non_mean between pressure and control groups\n",
    "sample_mean_pressure = data_male_pressure_non_mean['non_mean'].mean()\n",
    "se_pressure = data_male_pressure_non_mean['non_mean'].std() / np.sqrt(len(data_male_pressure_non_mean))\n",
    "\n",
    "z_score_pressure = (sample_mean_pressure - data_male_control_non_mean['non_mean'].mean()) / se_pressure\n",
    "p_value_pressure = 2 * (1 - stats.norm.cdf(abs(z_score_pressure)))\n",
    "\n",
    "print(f\"Z-Score (Pressure vs Control, non_mean): {z_score_pressure}, P-Value: {p_value_pressure}\")\n",
    "\n",
    "# Z-test for pressure group comparing non_mean to con_mean\n",
    "sample_mean_pressure = data_male_pressure_non_mean['non_mean'].mean()\n",
    "se_pressure = data_male_pressure_non_mean['non_mean'].std() / np.sqrt(len(data_male_pressure_non_mean))\n",
    "\n",
    "# Compare pressure group 'non_mean' to control group's 'con_mean'\n",
    "z_score_con_pressure = (sample_mean_pressure - data_male_control_con_mean['con_mean'].mean()) / se_pressure\n",
    "p_value_con_pressure = 2 * (1 - stats.norm.cdf(abs(z_score_con_pressure)))\n",
    "\n",
    "print(f\"Z-Score (Pressure non_mean vs Control con_mean): {z_score_con_pressure}, P-Value: {p_value_con_pressure}\")\n",
    "\n",
    "\n",
    "#repeat for unclear consent scenarios\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Subset the data for males by group number (control, info, pressure)\n",
    "data_male_control = data_male[data_male['group_number'] == 'control']\n",
    "data_male_info = data_male[data_male['group_number'] == 'info']\n",
    "data_male_pressure = data_male[data_male['group_number'] == 'pressure']\n",
    "\n",
    "# Filter out rows with missing values for unc_max\n",
    "data_male_control_unc_max = data_male_control.dropna(subset=['unc_max'])\n",
    "data_male_info_unc_max = data_male_info.dropna(subset=['unc_max'])\n",
    "data_male_pressure_unc_max = data_male_pressure.dropna(subset=['unc_max'])\n",
    "\n",
    "# Print the means of unc_max for control, info, and pressure groups\n",
    "print(f\"Mean (Control, unc_max): {data_male_control_unc_max['unc_max'].mean()}\")\n",
    "print(f\"Mean (Info, unc_max): {data_male_info_unc_max['unc_max'].mean()}\")\n",
    "print(f\"Mean (Pressure, unc_max): {data_male_pressure_unc_max['unc_max'].mean()}\")\n",
    "\n",
    "# Z-test: Compare unc_max between info and control groups\n",
    "sample_mean_info = data_male_info_unc_max['unc_max'].mean()\n",
    "se_info = data_male_info_unc_max['unc_max'].std() / np.sqrt(len(data_male_info_unc_max))\n",
    "\n",
    "z_score_info = (sample_mean_info - data_male_control_unc_max['unc_max'].mean()) / se_info\n",
    "p_value_info = 2 * (1 - stats.norm.cdf(abs(z_score_info)))\n",
    "\n",
    "print(f\"Z-Score (Info vs Control, unc_max): {z_score_info}, P-Value: {p_value_info}\")\n",
    "\n",
    "# Z-test: Compare unc_max between pressure and control groups\n",
    "sample_mean_pressure = data_male_pressure_unc_max['unc_max'].mean()\n",
    "se_pressure = data_male_pressure_unc_max['unc_max'].std() / np.sqrt(len(data_male_pressure_unc_max))\n",
    "\n",
    "z_score_pressure = (sample_mean_pressure - data_male_control_unc_max['unc_max'].mean()) / se_pressure\n",
    "p_value_pressure = 2 * (1 - stats.norm.cdf(abs(z_score_pressure)))\n",
    "\n",
    "print(f\"Z-Score (Pressure vs Control, unc_max): {z_score_pressure}, P-Value: {p_value_pressure}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b997a8f3-a095-4d8d-9b42-d7ccf0d1b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeating analysis for women\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Subset the data for females by group number (control, info, pressure)\n",
    "data_female_control = data_female[data_female['group_number'] == 'control']\n",
    "data_female_info = data_female[data_female['group_number'] == 'info']\n",
    "data_female_pressure = data_female[data_female['group_number'] == 'pressure']\n",
    "\n",
    "# Function to calculate z-scores and p-values\n",
    "def calculate_z_test(info_mean, control_mean, se):\n",
    "    z_score = (info_mean - control_mean) / se\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
    "    return z_score, p_value\n",
    "\n",
    "### 1. Agg Mean Analysis for Females\n",
    "\n",
    "# Filter out rows with missing values for agg_mean\n",
    "data_female_control_agg = data_female_control.dropna(subset=['agg_mean'])\n",
    "data_female_info_agg = data_female_info.dropna(subset=['agg_mean'])\n",
    "data_female_pressure_agg = data_female_pressure.dropna(subset=['agg_mean'])\n",
    "\n",
    "# Print the means of agg_mean for control, info, and pressure groups\n",
    "print(f\"Mean (Control, agg_mean): {data_female_control_agg['agg_mean'].mean()}\")\n",
    "print(f\"Mean (Info, agg_mean): {data_female_info_agg['agg_mean'].mean()}\")\n",
    "print(f\"Mean (Pressure, agg_mean): {data_female_pressure_agg['agg_mean'].mean()}\")\n",
    "\n",
    "# Z-test: Compare agg_mean between info and control groups\n",
    "se_info = data_female_info_agg['agg_mean'].std() / np.sqrt(len(data_female_info_agg))\n",
    "z_score_info, p_value_info = calculate_z_test(data_female_info_agg['agg_mean'].mean(), data_female_control_agg['agg_mean'].mean(), se_info)\n",
    "print(f\"Z-Score (Info vs Control, agg_mean): {z_score_info}, P-Value: {p_value_info}\")\n",
    "\n",
    "# Z-test: Compare agg_mean between pressure and control groups\n",
    "se_pressure = data_female_pressure_agg['agg_mean'].std() / np.sqrt(len(data_female_pressure_agg))\n",
    "z_score_pressure, p_value_pressure = calculate_z_test(data_female_pressure_agg['agg_mean'].mean(), data_female_control_agg['agg_mean'].mean(), se_pressure)\n",
    "print(f\"Z-Score (Pressure vs Control, agg_mean): {z_score_pressure}, P-Value: {p_value_pressure}\")\n",
    "\n",
    "### 2. Con Mean Analysis for Females\n",
    "\n",
    "# Filter out rows with missing values for con_mean\n",
    "data_female_control_con = data_female_control.dropna(subset=['con_mean'])\n",
    "data_female_info_con = data_female_info.dropna(subset=['con_mean'])\n",
    "data_female_pressure_con = data_female_pressure.dropna(subset=['con_mean'])\n",
    "\n",
    "# Print the means of con_mean for control, info, and pressure groups\n",
    "print(f\"Mean (Control, con_mean): {data_female_control_con['con_mean'].mean()}\")\n",
    "print(f\"Mean (Info, con_mean): {data_female_info_con['con_mean'].mean()}\")\n",
    "print(f\"Mean (Pressure, con_mean): {data_female_pressure_con['con_mean'].mean()}\")\n",
    "\n",
    "# Z-test: Compare con_mean between info and control groups\n",
    "se_info_con = data_female_info_con['con_mean'].std() / np.sqrt(len(data_female_info_con))\n",
    "z_score_info_con, p_value_info_con = calculate_z_test(data_female_info_con['con_mean'].mean(), data_female_control_con['con_mean'].mean(), se_info_con)\n",
    "print(f\"Z-Score (Info vs Control, con_mean): {z_score_info_con}, P-Value: {p_value_info_con}\")\n",
    "\n",
    "# Z-test: Compare con_mean between pressure and control groups\n",
    "se_pressure_con = data_female_pressure_con['con_mean'].std() / np.sqrt(len(data_female_pressure_con))\n",
    "z_score_pressure_con, p_value_pressure_con = calculate_z_test(data_female_pressure_con['con_mean'].mean(), data_female_control_con['con_mean'].mean(), se_pressure_con)\n",
    "print(f\"Z-Score (Pressure vs Control, con_mean): {z_score_pressure_con}, P-Value: {p_value_pressure_con}\")\n",
    "\n",
    "### 3. Non Mean Analysis for Females\n",
    "\n",
    "# Filter out rows with missing values for non_mean\n",
    "data_female_control_non = data_female_control.dropna(subset=['non_mean'])\n",
    "data_female_info_non = data_female_info.dropna(subset=['non_mean'])\n",
    "data_female_pressure_non = data_female_pressure.dropna(subset=['non_mean'])\n",
    "\n",
    "# Print the means of non_mean for control, info, and pressure groups\n",
    "print(f\"Mean (Control, non_mean): {data_female_control_non['non_mean'].mean()}\")\n",
    "print(f\"Mean (Info, non_mean): {data_female_info_non['non_mean'].mean()}\")\n",
    "print(f\"Mean (Pressure, non_mean): {data_female_pressure_non['non_mean'].mean()}\")\n",
    "\n",
    "# Z-test: Compare non_mean between info and control groups\n",
    "se_info_non = data_female_info_non['non_mean'].std() / np.sqrt(len(data_female_info_non))\n",
    "z_score_info_non, p_value_info_non = calculate_z_test(data_female_info_non['non_mean'].mean(), data_female_control_non['non_mean'].mean(), se_info_non)\n",
    "print(f\"Z-Score (Info vs Control, non_mean): {z_score_info_non}, P-Value: {p_value_info_non}\")\n",
    "\n",
    "# Z-test: Compare non_mean between pressure and control groups\n",
    "se_pressure_non = data_female_pressure_non['non_mean'].std() / np.sqrt(len(data_female_pressure_non))\n",
    "z_score_pressure_non, p_value_pressure_non = calculate_z_test(data_female_pressure_non['non_mean'].mean(), data_female_control_non['non_mean'].mean(), se_pressure_non)\n",
    "print(f\"Z-Score (Pressure vs Control, non_mean): {z_score_pressure_non}, P-Value: {p_value_pressure_non}\")\n",
    "\n",
    "### 4. Unclear Mean Analysis for Females\n",
    "\n",
    "# Filter out rows with missing values for unc_max\n",
    "data_female_control_unc_max = data_female_control.dropna(subset=['unc_max'])\n",
    "data_female_info_unc_max = data_female_info.dropna(subset=['unc_max'])\n",
    "data_female_pressure_unc_max = data_female_pressure.dropna(subset=['unc_max'])\n",
    "\n",
    "# Print the means of unc_max for control, info, and pressure groups\n",
    "print(f\"Mean (Control, unc_max): {data_female_control_unc_max['unc_max'].mean()}\")\n",
    "print(f\"Mean (Info, unc_max): {data_female_info_unc_max['unc_max'].mean()}\")\n",
    "print(f\"Mean (Pressure, unc_max): {data_female_pressure_unc_max['unc_max'].mean()}\")\n",
    "\n",
    "# Z-test: Compare unc_max between info and control groups\n",
    "se_info_unc_max = data_female_info_unc_max['unc_max'].std() / np.sqrt(len(data_female_info_unc_max))\n",
    "z_score_info_unc_max, p_value_info_unc_max = calculate_z_test(data_female_info_unc_max['unc_max'].mean(), data_female_control_unc_max['unc_max'].mean(), se_info_unc_max)\n",
    "print(f\"Z-Score (Info vs Control, unc_max): {z_score_info_unc_max}, P-Value: {p_value_info_unc_max}\")\n",
    "\n",
    "# Z-test: Compare unc_max between pressure and control groups\n",
    "se_pressure_unc_max = data_female_pressure_unc_max['unc_max'].std() / np.sqrt(len(data_female_pressure_unc_max))\n",
    "z_score_pressure_unc_max, p_value_pressure_unc_max = calculate_z_test(data_female_pressure_unc_max['unc_max'].mean(), data_female_control_unc_max['unc_max'].mean(), se_pressure_unc_max)\n",
    "print(f\"Z-Score (Pressure vs Control, unc_max): {z_score_pressure_unc_max}, P-Value: {p_value_pressure_unc_max}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17776cfc-dd04-4f80-9170-8dbac2a9df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate analysis based on how frequency participant consumes pornography outside the experiment\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Split into low and high frequency for 'freq_porn'\n",
    "data_freq = data.dropna(subset=['freq_porn'])\n",
    "\n",
    "# Split into low and high frequency based on 'freq_porn'\n",
    "data_low_freq = data_freq[data_freq['freq_porn'] <= 2].copy()\n",
    "data_high_freq = data_freq[data_freq['freq_porn'] >= 3].copy()\n",
    "\n",
    "# Print the low and high frequency dataframes\n",
    "print(\"Low Frequency Data:\\n\", data_low_freq.head())\n",
    "print(\"High Frequency Data:\\n\", data_high_freq.head())\n",
    "\n",
    "### Z-Test Function ###\n",
    "def calculate_z_test(group1_mean, group2_mean, se):\n",
    "    z_score = (group1_mean - group2_mean) / se\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
    "    return z_score, p_value\n",
    "\n",
    "### 2. Mean comparison for agg_mean (low freq_porn only)\n",
    "\n",
    "# Subset data by group_number for low freq_porn\n",
    "data_low_control = data_low_freq[data_low_freq['group_number'] == 'control'].dropna(subset=['agg_mean'])\n",
    "data_low_info = data_low_freq[data_low_freq['group_number'] == 'info'].dropna(subset=['agg_mean'])\n",
    "data_low_pressure = data_low_freq[data_low_freq['group_number'] == 'pressure'].dropna(subset=['agg_mean'])\n",
    "\n",
    "# Print means for agg_mean\n",
    "print(f\"Mean (Control, agg_mean): {data_low_control['agg_mean'].mean()}\")\n",
    "print(f\"Mean (Info, agg_mean): {data_low_info['agg_mean'].mean()}\")\n",
    "print(f\"Mean (Pressure, agg_mean): {data_low_pressure['agg_mean'].mean()}\")\n",
    "\n",
    "# Z-test between info and control groups\n",
    "se_info = data_low_info['agg_mean'].std() / np.sqrt(len(data_low_info))\n",
    "z_score_info, p_value_info = calculate_z_test(data_low_info['agg_mean'].mean(), data_low_control['agg_mean'].mean(), se_info)\n",
    "print(f\"Z-Score (Info vs Control, agg_mean): {z_score_info}, P-Value: {p_value_info}\")\n",
    "\n",
    "# Z-test between pressure and control groups\n",
    "se_pressure = data_low_pressure['agg_mean'].std() / np.sqrt(len(data_low_pressure))\n",
    "z_score_pressure, p_value_pressure = calculate_z_test(data_low_pressure['agg_mean'].mean(), data_low_control['agg_mean'].mean(), se_pressure)\n",
    "print(f\"Z-Score (Pressure vs Control, agg_mean): {z_score_pressure}, P-Value: {p_value_pressure}\")\n",
    "\n",
    "### 3. Mean comparison for unc_max (low freq_porn only)\n",
    "\n",
    "# Subset data for unc_max\n",
    "data_low_control_unc = data_low_control.dropna(subset=['unc_max'])\n",
    "data_low_info_unc = data_low_info.dropna(subset=['unc_max'])\n",
    "data_low_pressure_unc = data_low_pressure.dropna(subset=['unc_max'])\n",
    "\n",
    "# Print means for unc_max\n",
    "print(f\"Mean (Control, unc_max): {data_low_control_unc['unc_max'].mean()}\")\n",
    "print(f\"Mean (Info, unc_max): {data_low_info_unc['unc_max'].mean()}\")\n",
    "print(f\"Mean (Pressure, unc_max): {data_low_pressure_unc['unc_max'].mean()}\")\n",
    "\n",
    "# Z-test for unc_max between info and control groups\n",
    "se_info_unc = data_low_info_unc['unc_max'].std() / np.sqrt(len(data_low_info_unc))\n",
    "z_score_info_unc, p_value_info_unc = calculate_z_test(data_low_info_unc['unc_max'].mean(), data_low_control_unc['unc_max'].mean(), se_info_unc)\n",
    "print(f\"Z-Score (Info vs Control, unc_max): {z_score_info_unc}, P-Value: {p_value_info_unc}\")\n",
    "\n",
    "# Z-test for unc_max between pressure and control groups\n",
    "se_pressure_unc = data_low_pressure_unc['unc_max'].std() / np.sqrt(len(data_low_pressure_unc))\n",
    "z_score_pressure_unc, p_value_pressure_unc = calculate_z_test(data_low_pressure_unc['unc_max'].mean(), data_low_control_unc['unc_max'].mean(), se_pressure_unc)\n",
    "print(f\"Z-Score (Pressure vs Control, unc_max): {z_score_pressure_unc}, P-Value: {p_value_pressure_unc}\")\n",
    "\n",
    "### 4. Mean comparison for agg_mean (high freq_porn only)\n",
    "\n",
    "# Subset data by group_number for high frequency\n",
    "data_high_control = data_high_freq[data_high_freq['group_number'] == 'control'].dropna(subset=['agg_mean'])\n",
    "data_high_info = data_high_freq[data_high_freq['group_number'] == 'info'].dropna(subset=['agg_mean'])\n",
    "data_high_pressure = data_high_freq[data_high_freq['group_number'] == 'pressure'].dropna(subset=['agg_mean'])\n",
    "\n",
    "# Print means for agg_mean\n",
    "print(f\"Mean (Control, agg_mean): {data_high_control['agg_mean'].mean()}\")\n",
    "print(f\"Mean (Info, agg_mean): {data_high_info['agg_mean'].mean()}\")\n",
    "print(f\"Mean (Pressure, agg_mean): {data_high_pressure['agg_mean'].mean()}\")\n",
    "\n",
    "# Z-test for agg_mean between info and control groups\n",
    "se_info_high = data_high_info['agg_mean'].std() / np.sqrt(len(data_high_info))\n",
    "z_score_info_high, p_value_info_high = calculate_z_test(data_high_info['agg_mean'].mean(), data_high_control['agg_mean'].mean(), se_info_high)\n",
    "print(f\"Z-Score (Info vs Control, agg_mean): {z_score_info_high}, P-Value: {p_value_info_high}\")\n",
    "\n",
    "# Z-test for agg_mean between pressure and control groups\n",
    "se_pressure_high = data_high_pressure['agg_mean'].std() / np.sqrt(len(data_high_pressure))\n",
    "z_score_pressure_high, p_value_pressure_high = calculate_z_test(data_high_pressure['agg_mean'].mean(), data_high_control['agg_mean'].mean(), se_pressure_high)\n",
    "print(f\"Z-Score (Pressure vs Control, agg_mean): {z_score_pressure_high}, P-Value: {p_value_pressure_high}\")\n",
    "\n",
    "### 5. Z-tests for con_mean and non_mean (high freq_porn only)\n",
    "\n",
    "# Subset data for con_mean and non_mean (high frequency)\n",
    "data_high_control_con = data_high_control.dropna(subset=['con_mean'])\n",
    "data_high_info_con = data_high_info.dropna(subset=['con_mean'])\n",
    "data_high_pressure_con = data_high_pressure.dropna(subset=['con_mean'])\n",
    "\n",
    "# Z-test for con_mean between info and control groups\n",
    "se_info_con_high = data_high_info_con['con_mean'].std() / np.sqrt(len(data_high_info_con))\n",
    "z_score_info_con_high, p_value_info_con_high = calculate_z_test(data_high_info_con['con_mean'].mean(), data_high_control_con['con_mean'].mean(), se_info_con_high)\n",
    "print(f\"Z-Score (Info vs Control, con_mean): {z_score_info_con_high}, P-Value: {p_value_info_con_high}\")\n",
    "\n",
    "# Z-test for con_mean between pressure and control groups\n",
    "se_pressure_con_high = data_high_pressure_con['con_mean'].std() / np.sqrt(len(data_high_pressure_con))\n",
    "z_score_pressure_con_high, p_value_pressure_con_high = calculate_z_test(data_high_pressure_con['con_mean'].mean(), data_high_control_con['con_mean'].mean(), se_pressure_con_high)\n",
    "print(f\"Z-Score (Pressure vs Control, con_mean): {z_score_pressure_con_high}, P-Value: {p_value_pressure_con_high}\")\n",
    "\n",
    "# Repeat similar Z-tests for non_mean and unc_max\n",
    "data_high_control_non = data_high_control.dropna(subset=['non_mean'])\n",
    "data_high_info_non = data_high_info.dropna(subset=['non_mean'])\n",
    "data_high_pressure_non = data_high_pressure.dropna(subset=['non_mean'])\n",
    "\n",
    "se_info_non_high = data_high_info_non['non_mean'].std() / np.sqrt(len(data_high_info_non))\n",
    "z_score_info_non_high, p_value_info_non_high = calculate_z_test(data_high_info_non['non_mean'].mean(), data_high_control_non['non_mean'].mean(), se_info_non_high)\n",
    "print(f\"Z-Score (Info vs Control, non_mean): {z_score_info_non_high}, P-Value: {p_value_info_non_high}\")\n",
    "\n",
    "# Z-test for non_mean between pressure and control groups\n",
    "se_pressure_non_high = data_high_pressure_non['non_mean'].std() / np.sqrt(len(data_high_pressure_non))\n",
    "z_score_pressure_non_high, p_value_pressure_non_high = calculate_z_test(data_high_pressure_non['non_mean'].mean(), data_high_control_non['non_mean'].mean(), se_pressure_non_high)\n",
    "print(f\"Z-Score (Pressure vs Control, non_mean): {z_score_pressure_non_high}, P-Value: {p_value_pressure_non_high}\")\n",
    "\n",
    "### 6. Linear Regression Comparisons for agg_mean and freq_porn\n",
    "\n",
    "# Linear regression for control group\n",
    "regression_control = sm.OLS.from_formula('agg_mean ~ gender_identity + freq_porn', data=data_control).fit()\n",
    "print(regression_control.summary())\n",
    "\n",
    "# Linear regression for info group\n",
    "regression_info = sm.OLS.from_formula('agg_mean ~ gender_identity + freq_porn', data=data_info).fit()\n",
    "print(regression_info.summary())\n",
    "\n",
    "# Linear regression for pressure group\n",
    "regression_pressure = sm.OLS.from_formula('agg_mean ~ gender_identity + freq_porn', data=data_pressure).fit()\n",
    "print(regression_pressure.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628edb7-004c-44ce-9818-45fbce904597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "# 1. Linear regression to test if gender differences are fully explained by freq_porn\n",
    "model = ols('agg_mean ~ treatment + gender_identity + freq_porn', data=data).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# Test the significance of the gender coefficient after controlling for freq_porn\n",
    "model_controlled = ols('agg_mean ~ treatment + freq_porn + gender_identity:freq_porn', data=data).fit()\n",
    "anova_results = sm.stats.anova_lm(model, model_controlled)\n",
    "print(anova_results)\n",
    "\n",
    "# 2. Linear regressions for male and female participants separately\n",
    "# For males\n",
    "print(ols('agg_mean ~ freq_porn', data=data_male).fit().summary())\n",
    "\n",
    "# For females\n",
    "print(ols('agg_mean ~ freq_porn', data=data_female).fit().summary())\n",
    "\n",
    "# 3. Double difference model: Interaction between guilt and treatment, for males and females\n",
    "\n",
    "# Subset data for males and females\n",
    "data_male = data[data['gender_identity'] == 'male']\n",
    "data_female = data[data['gender_identity'] == 'female']\n",
    "\n",
    "# Create new 'guilt' and 'treat_yes' columns based on thresholds\n",
    "data_male['guilt'] = np.where(data_male['guilt_feelings'] <= 2, 0, 1)\n",
    "data_female['guilt'] = np.where(data_female['guilt_feelings'] <= 2, 0, 1)\n",
    "data_male['treat_yes'] = np.where((data_male['group_number'] == 'info') | (data_male['group_number'] == 'pressure'), 1, 0)\n",
    "data_female['treat_yes'] = np.where((data_female['group_number'] == 'info') | (data_female['group_number'] == 'pressure'), 1, 0)\n",
    "\n",
    "# Run linear regression models with interaction terms\n",
    "print(ols('agg_mean ~ treat_yes + guilt + treat_yes*guilt', data=data_male).fit().summary())\n",
    "print(ols('agg_mean ~ treat_yes + guilt + treat_yes*guilt', data=data_female).fit().summary())\n",
    "\n",
    "# 4. Interaction between guilt and group_number for men\n",
    "print(ols('agg_mean ~ guilt * group_number', data=data_male).fit().summary())\n",
    "\n",
    "# 5. Triple difference model to test interactions between guilt, treatment, and gender\n",
    "data['guilt'] = np.where(data['guilt_feelings'] <= 2, 0, 1)\n",
    "data['treat_yes'] = np.where((data['group_number'] == 'info') | (data['group_number'] == 'pressure'), 1, 0)\n",
    "\n",
    "# Linear regression model with triple interaction terms\n",
    "print(ols('agg_mean ~ guilt + treat_yes + guilt*treat_yes + gender_identity + gender_identity*treat_yes + treat_yes*guilt*gender_identity', data=data).fit().summary())\n",
    "\n",
    "# 6. Correlation between freq_porn and comfort with non-consensual scenarios\n",
    "non_mean_correlation = data[['non_mean', 'freq_porn']].corr().loc['non_mean', 'freq_porn']\n",
    "unc_max_correlation = data[['unc_max', 'freq_porn']].corr().loc['unc_max', 'freq_porn']\n",
    "\n",
    "print(f\"Correlation between non_mean and freq_porn: {non_mean_correlation}\")\n",
    "print(f\"Correlation between unc_max and freq_porn: {unc_max_correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6231e-b0e2-4276-87b6-fb2f28a99a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
